{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ax3t641WR_5"
   },
   "source": [
    "### **Data Masking Evaluation (summary)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T17:30:47.837028Z",
     "start_time": "2025-08-06T17:30:42.944620Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ZSpfjx701H-",
    "outputId": "04ced276-3c54-466a-b9c3-146f2ee9ad53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (4.55.0)\n",
      "Requirement already satisfied: torch in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T17:30:51.190947Z",
     "start_time": "2025-08-06T17:30:47.903562Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hwey6TGLCC8N",
    "outputId": "f1ff3cc6-0a39-44dc-d3a8-36bd439a405e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (0.27.1)\n",
      "Requirement already satisfied: Levenshtein==0.27.1 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from python-Levenshtein) (0.27.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from Levenshtein==0.27.1->python-Levenshtein) (3.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T17:57:27.463631Z",
     "start_time": "2025-08-28T17:57:25.642197Z"
    },
    "id": "_QZktNg4ayNd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8HQzX5JPdfV"
   },
   "source": [
    "#### Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T18:00:05.958444Z",
     "start_time": "2025-08-28T17:59:56.308675Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rafau\\Documents\\PythonProjects\\DataSanitization\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"hf://datasets/AGBonnet/augmented-clinical-notes/augmented_notes_30K.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T18:00:10.080623Z",
     "start_time": "2025-08-28T18:00:09.976846Z"
    },
    "id": "Uz4QGKmYPZpf"
   },
   "outputs": [],
   "source": [
    "df_masked = pd.read_csv(\"../datasets/summaries/summaries_masked_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T18:00:12.006289Z",
     "start_time": "2025-08-28T18:00:11.997291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.0',\n",
       " '10 years',\n",
       " '4.4 cm',\n",
       " '65-year-old',\n",
       " 'Gore',\n",
       " 'He',\n",
       " 'His',\n",
       " 'a covered, self - expanding stent',\n",
       " 'a less invasive treatment',\n",
       " 'a pulsating mass',\n",
       " 'a saccular dilatation in the tibioperoneal trunk',\n",
       " 'a swelling of the posterior surface',\n",
       " 'active infections',\n",
       " 'an intestinal tumor',\n",
       " 'bacterial endocarditis',\n",
       " 'difficulties',\n",
       " 'ec',\n",
       " 'gore viabahn',\n",
       " 'his',\n",
       " 'male',\n",
       " 'one',\n",
       " 'one varicose veins surgery',\n",
       " 'reduction of the distal diameter',\n",
       " 'the aneurysm',\n",
       " 'the aneurysm sac',\n",
       " 'the inflammatory / infectious pathophysiology',\n",
       " 'the pseudoaneurysm',\n",
       " 'the symptoms',\n",
       " 'treatment',\n",
       " 'two',\n",
       " 'two abdominal operations',\n",
       " 'two mitral valve replacement operations'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(df_masked['sensitive_entity_note'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEVFTfmZUBUL"
   },
   "source": [
    "### **Average Levenshtein Index of Dissimilarity (ALID)**\n",
    "<br>\n",
    "\n",
    "Compute LSI for each sensitive entity of observation:\n",
    "$$\n",
    "LSI = \\max_{j=1}^{L-e} LR_a(se_i, w_j)\n",
    "$$\n",
    "\n",
    "Next compute the average of each LSI calculated before:\n",
    "$$\n",
    "ALID = (1 - \\langle S \\rangle) \\times 100\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T18:00:17.648618Z",
     "start_time": "2025-08-28T18:00:17.642653Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXmS-pUrC8PM",
    "outputId": "6bf6ab4d-8abf-4a3c-cb01-426cb0efecf0"
   },
   "outputs": [],
   "source": [
    "def LRa(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    Levenshtein Ratio:\n",
    "    LRa(a, b) = 1 - LD(a, b) / max(len(a), len(b))\n",
    "    \"\"\"\n",
    "    len_a = len(a)\n",
    "    len_b = len(b)\n",
    "    if max(len_a, len_b) == 0:\n",
    "        return 1.0  # both strings are empty\n",
    "    ld = Levenshtein.distance(a, b)\n",
    "    return 1 - (ld / max(len_a, len_b))\n",
    "\n",
    "def compute_lsi(entity: str, text_anon: str) -> float:\n",
    "    e_len = len(entity)\n",
    "    max_lra = 0.0\n",
    "    for i in range(len(text_anon) - e_len + 1):\n",
    "        window = text_anon[i:i + e_len]\n",
    "        lra = LRa(entity, window)\n",
    "        max_lra = max(max_lra, lra)\n",
    "    return max_lra\n",
    "\n",
    "def compute_average_levenshtein_index_of_dissimilarity(masked_note: str, sensitive_entity_note: list) -> float:\n",
    "    lsi_values = [compute_lsi(ent, masked_note) for ent in sensitive_entity_note]\n",
    "    average_lsi = np.mean(lsi_values)\n",
    "    ALID = (1 - average_lsi) * 100\n",
    "    return ALID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T18:00:32.849967Z",
     "start_time": "2025-08-28T18:00:32.792221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALID: 51.13%\n"
     ]
    }
   ],
   "source": [
    "alid = compute_average_levenshtein_index_of_dissimilarity(df_masked['summary'][0], ast.literal_eval(df_masked['sensitive_entity_note'][0]))\n",
    "print(f\"ALID: {alid:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3suaj0HUIxv"
   },
   "source": [
    "### **Levenshtein Recall (LR)**\n",
    "\n",
    "<br>\n",
    "To calculate LR, each LSI in S is compared to a selected similarity threshold, $ths$, set to 0.85:\n",
    "\n",
    "$$\n",
    "LR@ths = \\frac{ \\sum_{i=1}^{l} (S_i < ths) }{l} \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T18:00:53.211541Z",
     "start_time": "2025-08-28T18:00:53.207060Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b4K6-AEQFUA",
    "outputId": "05677b87-1490-4a46-ad32-98bc7c8cdcaf"
   },
   "outputs": [],
   "source": [
    "def compute_LR_ths(masked_note: str, sensitive_entity_note: list, ths=0.85) -> float:\n",
    "    LSI_scores = []\n",
    "    for entity in sensitive_entity_note:\n",
    "        LSI_scores.append(compute_lsi(entity, masked_note))\n",
    "\n",
    "    # Compute recall\n",
    "    num_deidentified = sum(1 for s in LSI_scores if s < ths)\n",
    "    recall = (num_deidentified / len(sensitive_entity_note)) * 100\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T18:01:02.727567Z",
     "start_time": "2025-08-28T18:01:02.670690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Recall: 93.75%\n"
     ]
    }
   ],
   "source": [
    "lr_score = compute_LR_ths(df_masked['summary'][0], ast.literal_eval(df_masked['sensitive_entity_note'][0]))\n",
    "print(f\"Levenshtein Recall: {lr_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrpd13nOWG0q"
   },
   "source": [
    "### **Levenshtein Recall Quasi Identifier (LRQI)**\n",
    "\n",
    "The LRQI is calculated similarly to the LR but only considering quasi-identifiers.\n",
    "\n",
    "$$\n",
    "\\text{LRQI}@ths = \\frac{ \\sum_{k=1}^{l_{qi}} (S_k < ths) }{l_{qi}} \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T18:01:08.664928Z",
     "start_time": "2025-08-28T18:01:08.649645Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxfhW2ftWGik",
    "outputId": "71a794e6-a3d7-4a98-9ce5-42a862de0bcb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import difflib\n",
    "\n",
    "def compute_LRQI_ths(masked_note: str, sensitive_entity_note: list, json_summary: str, ths=0.85) -> float:\n",
    "\n",
    "    def extract_qi_values_from_json(json_str: str) -> list:\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            return []\n",
    "\n",
    "        qi_values = []\n",
    "\n",
    "        # Extract patient information\n",
    "        pi = data.get(\"patient information\", {})\n",
    "        for key in [\"age\", \"sex\", \"ethnicity\", \"occupation\", \"socio economic context\"]:\n",
    "            v = pi.get(key)\n",
    "            if v and v.lower() != \"none\":\n",
    "                qi_values.append(v)\n",
    "\n",
    "        # Extract patient medical history\n",
    "        pmh = data.get(\"patient medical history\", {})\n",
    "\n",
    "        if isinstance(pmh, dict):\n",
    "\n",
    "            # Physiological context\n",
    "            phsy = pmh.get(\"physiological context\")\n",
    "            if phsy is not None:\n",
    "                if isinstance(phsy, str):\n",
    "                    phsy = phsy.split(\",\")\n",
    "                elif isinstance(phsy, dict):\n",
    "                    phsy = list(phsy.values())\n",
    "                else:\n",
    "                    phsy = []\n",
    "                for p in phsy:\n",
    "                    qi_values.append(p)\n",
    "\n",
    "            # Psychological context\n",
    "            psych = pmh.get(\"psychological context\")\n",
    "            if psych is not None:\n",
    "                if isinstance(psych, str):\n",
    "                    psych = psych.split(\",\")\n",
    "                elif isinstance(psych, dict):\n",
    "                    psych = list(psych.values())\n",
    "                else:\n",
    "                    psych = []\n",
    "                for p in psych:\n",
    "                    qi_values.append(p)\n",
    "\n",
    "        elif isinstance(pmh, str) and pmh.lower() != \"none\":\n",
    "            qi_values.append(pmh)\n",
    "\n",
    "        # Extract symptoms\n",
    "        for s in data.get(\"symptoms\", []):\n",
    "            if s is not None and isinstance(s, dict):\n",
    "                if s.get(\"time\") and s[\"time\"].lower() != \"none\":\n",
    "                    qi_values.append(s[\"time\"])\n",
    "                if s.get(\"location\") and s[\"location\"].lower() != \"none\":\n",
    "                    qi_values.append(s[\"location\"])\n",
    "                if s.get(\"name of symptom\") and s[\"name of symptom\"].lower() != \"none\":\n",
    "                    for symptom in s.get(\"name of symptom\").split(\",\"):\n",
    "                        qi_values.append(symptom)\n",
    "\n",
    "        # Extract treatments\n",
    "        for t in data.get(\"treatments\", []):\n",
    "            if t is not None and isinstance(t, dict):\n",
    "                if t.get(\"name\") and t[\"name\"].lower() != \"none\":\n",
    "                    qi_values.append(t[\"name\"])\n",
    "                if t.get(\"dosage\") and t[\"dosage\"].lower() != \"none\":\n",
    "                    qi_values.append(t[\"dosage\"])\n",
    "                if t.get(\"reaction to treatment\") and t[\"reaction to treatment\"].lower() != \"none\":\n",
    "                    for reaction in t.get(\"reaction to treatment\").split(\",\"):\n",
    "                        qi_values.append(reaction)\n",
    "                if t.get(\"time\") and t[\"time\"].lower() != \"none\":\n",
    "                    qi_values.append(t[\"time\"])\n",
    "        return qi_values\n",
    "\n",
    "\n",
    "    def filter_qi_entities(sensitive_entities, qi_values, threshold=0.5):\n",
    "        filtered = []\n",
    "        se_lower = [e.lower() for e in sensitive_entities]\n",
    "\n",
    "        for qi in qi_values:\n",
    "            qi_low = qi.lower()\n",
    "            for ent in se_lower:\n",
    "                if qi_low in ent or ent in qi_low:\n",
    "                    filtered.append(ent)\n",
    "\n",
    "        for qi in qi_values:\n",
    "            qi_low = qi.lower()\n",
    "            for se in sensitive_entities:\n",
    "                se_low = se.lower()\n",
    "                ratio = difflib.SequenceMatcher(None, qi_low, se_low).ratio()\n",
    "                if ratio >= threshold:\n",
    "                    filtered.append(se)\n",
    "\n",
    "        return list(set(filtered))\n",
    "\n",
    "    qi_vals = extract_qi_values_from_json(json_summary)\n",
    "\n",
    "    if len(qi_vals) == 0: return -1\n",
    "\n",
    "    qi_entities = filter_qi_entities(sensitive_entity_note, qi_vals)\n",
    "\n",
    "    return compute_LR_ths(masked_note, qi_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T18:01:40.082923Z",
     "start_time": "2025-08-28T18:01:40.028408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Recall Quasi Identifiers: 87.50%\n"
     ]
    }
   ],
   "source": [
    "lr_score = compute_LRQI_ths(masked_note=df_masked['summary'][0], sensitive_entity_note=ast.literal_eval(df_masked['sensitive_entity_note'][0]),\n",
    "                            json_summary=df['summary'][df_masked['index'][0]])\n",
    "print(f\"Levenshtein Recall Quasi Identifiers: {lr_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mLErJpWUO5R"
   },
   "source": [
    "### **Jaccard Similarity Coefficient (JSC)**\n",
    "\n",
    "Let C11 be the number of classes where both representations have a value of 1 and C01 + C10 be the number of classes where the representations have different values:\n",
    "\n",
    "$$\n",
    "\\text{JSC@thb} = \\frac{C_{11}}{C_{11} + C_{01} + C_{10}} \\times 100\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T18:01:57.387546Z",
     "start_time": "2025-08-28T18:01:44.250599Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLh3mM4QC1rE",
    "outputId": "53456830-b761-4660-f09b-e7e84c9aee70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AkshatSurolia/ICD-10-Code-Prediction\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"AkshatSurolia/ICD-10-Code-Prediction\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "config = model.config\n",
    "\n",
    "def chunk_text_ids(text: str, max_tokens=512):\n",
    "    inputs = tokenizer(text, add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids']\n",
    "    max_len = max_tokens - 2  # reserved for special tokens\n",
    "    chunks = [input_ids[i:i+max_len] for i in range(0, len(input_ids), max_len)]\n",
    "    return chunks\n",
    "\n",
    "def aggregate_logits_from_chunks(text) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    chunks = chunk_text_ids(text, max_tokens=512)\n",
    "    total_logits = None\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # add CLS e SEP token id\n",
    "        input_ids = [tokenizer.cls_token_id] + chunk + [tokenizer.sep_token_id]\n",
    "        input_tensor = torch.tensor([input_ids])  # batch size 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor).logits.squeeze(0)  # [num_classes]\n",
    "\n",
    "        if total_logits is None:\n",
    "            total_logits = logits\n",
    "        else:\n",
    "            total_logits += logits  # sum logits\n",
    "\n",
    "    return total_logits\n",
    "\n",
    "def compute_jaccard_similarity_coefficient(original_note: str, masked_note: str, thb: float = 0.0005) -> float:\n",
    "\n",
    "    final_logits_original_note = aggregate_logits_from_chunks(original_note)\n",
    "    final_logits_masked_note = aggregate_logits_from_chunks(masked_note)\n",
    "\n",
    "    probs_original_note = F.softmax(final_logits_original_note, dim=0).squeeze()\n",
    "    probs_masked_note = F.softmax(final_logits_masked_note, dim=0).squeeze()\n",
    "\n",
    "    # list of indexes\n",
    "    # results = final_logits_original_note.detach().cpu().numpy()[0].argsort()[::-1][:10]\n",
    "    # [config.id2label[ids] for ids in results]\n",
    "\n",
    "    predicted_original_note = set(torch.where(probs_original_note > thb)[0].tolist())\n",
    "    predicted_masked_note = set(torch.where(probs_masked_note > thb)[0].tolist())\n",
    "\n",
    "    C11 = len(predicted_original_note & predicted_masked_note)\n",
    "    C01 = len(predicted_masked_note - predicted_original_note)\n",
    "    C10 = len(predicted_original_note - predicted_masked_note)\n",
    "\n",
    "    denom = C11 + C01 + C10\n",
    "\n",
    "    if denom == 0: return 100.0  # noone classed\n",
    "\n",
    "    return (C11 / denom) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T18:02:26.315864Z",
     "start_time": "2025-08-28T18:02:24.994448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSC Score: 15.00%\n"
     ]
    }
   ],
   "source": [
    "jsc_score = compute_jaccard_similarity_coefficient(df_masked['note'][0], df_masked['summary'][0])\n",
    "print(f\"JSC Score: {jsc_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7l2OtVdOUUfl"
   },
   "source": [
    "### **Normalized Soft Discounted Cumulative Gain (NSDCG@K)**\n",
    "\n",
    "**SDCG@K (Soft Discounted Cumulative Gain)** is a metric used to evaluate the ranking quality of the logits produced by a model on anonymized notes compared to the original ones.\n",
    "\n",
    "$$\n",
    "\\text{SDCG@K} = \\sum_{i=1}^{K} sdi \\cdot reli\n",
    "$$\n",
    "\n",
    "- $sd_i$: soft discount factor at position *i*\n",
    "- $rel_i$: relevance of the item at position *i*\n",
    "\n",
    "Given a vector of **sorted (descending) logits** \\( s \\), the soft discount at position *i* is computed using the **softmax** function:\n",
    "\n",
    "$$\n",
    "sdi = \\frac{e^{s_i}}{\\sum_{j=1}^{N} e^{s_j}}\n",
    "$$\n",
    "\n",
    "Where $N$ is the number of ICD-10 classes.\n",
    "\n",
    "The relevance of an item at position *i* in the original logits $z$ (i.e., logits from the original note ranked according to the anonymized note) is defined as:\n",
    "\n",
    "$$\n",
    "reli = e^{z_i}\n",
    "$$\n",
    "\n",
    "**NSDCG@K (Normalized SDCG)** is obtained by dividing the SDCG of the anonymized note by the SDCG of the ideal/original note and expressing the result as a percentage:\n",
    "\n",
    "$$\n",
    "\\text{NSDCG@K} = \\frac{\\text{SDCG@K}}{\\text{ISDCG@K}} \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T18:04:33.539293Z",
     "start_time": "2025-08-28T18:04:33.532068Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RPMYsFy6ohi",
    "outputId": "b14a8ea5-9837-4075-829e-0eb4059adc26"
   },
   "outputs": [],
   "source": [
    "def compute_normalized_soft_discounted_cumulative_gain(original_note: str, masked_note: str, k=10) -> float:\n",
    "\n",
    "    logits_orig = aggregate_logits_from_chunks(original_note)\n",
    "    logits_anon = aggregate_logits_from_chunks(masked_note)\n",
    "\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    # top-K indexes from logit of masked text\n",
    "    top_k_values, top_k_indices_anon = torch.topk(logits_anon, k)\n",
    "\n",
    "    # relevance: original logit ordered via top-K of masked text\n",
    "    relevances = np.exp(logits_orig[top_k_indices_anon])\n",
    "\n",
    "    # softmax discount of original logit ordered decreasing\n",
    "    sorted_logits_orig = np.sort(logits_orig)[::-1]\n",
    "    softmax_discounts = softmax(sorted_logits_orig[:k])\n",
    "\n",
    "    # compute SDCG\n",
    "    sdcg = np.sum(softmax_discounts * relevances.cpu().numpy())\n",
    "\n",
    "    # compute ISDCG (ideal)\n",
    "    ideal_relevances = np.exp(sorted_logits_orig[:k])\n",
    "    ideal_discounts = softmax(sorted_logits_orig[:k])\n",
    "    isdcg = np.sum(ideal_discounts * ideal_relevances)\n",
    "\n",
    "    nsdcg = (sdcg / isdcg) * 100 if isdcg != 0 else 0\n",
    "\n",
    "    return nsdcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T18:05:03.603862Z",
     "start_time": "2025-08-28T18:05:02.534881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSDCG@K Score: 41.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rafau\\AppData\\Local\\Temp\\ipykernel_69236\\2998378043.py:14: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  relevances = np.exp(logits_orig[top_k_indices_anon])\n"
     ]
    }
   ],
   "source": [
    "nsdcg_score = compute_normalized_soft_discounted_cumulative_gain(df_masked['note'][0], df_masked['summary'][0])\n",
    "print(f\"NSDCG@K Score: {nsdcg_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute metrics for all records in masked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T18:05:07.395438Z",
     "start_time": "2025-08-28T18:05:07.391057Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process the desired output.\n",
    "def process_metrics(id_note: int, ALID: float, LR: float, LRQI: float, JSC: float, NSDCG: float) -> dict:\n",
    "\n",
    "    return {\n",
    "    \"id_note\": int(id_note),\n",
    "    \"ALID\": round(float(ALID), 2),\n",
    "    \"LR\": round(float(LR), 2),\n",
    "    \"LRQI\": round(float(LRQI), 2),\n",
    "    \"JSC\": round(float(JSC), 2),\n",
    "    \"NSDCG\": round(float(NSDCG), 2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T18:56:29.243550Z",
     "start_time": "2025-08-28T18:05:44.860251Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note evaluation progress:   0%|          | 0/1000 [00:00<?, ?it/s]C:\\Users\\rafau\\AppData\\Local\\Temp\\ipykernel_69236\\2998378043.py:14: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  relevances = np.exp(logits_orig[top_k_indices_anon])\n",
      "Note evaluation progress: 100%|██████████| 1000/1000 [50:44<00:00,  3.04s/it] \n"
     ]
    }
   ],
   "source": [
    "# Check if exists test folder\n",
    "if 'test' not in os.listdir():\n",
    "    os.mkdir('test')\n",
    "\n",
    "# File to save masked note evaluation.\n",
    "file_path = f'./test/masked_note_eval.json'\n",
    "\n",
    "# Check if file exists.\n",
    "if os.path.exists(file_path):\n",
    "    # Empty file.\n",
    "    if os.stat(file_path).st_size == 0:\n",
    "        formatted_notes = []\n",
    "    else:\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            formatted_notes = data.get('notes', [])\n",
    "else:\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump({\"notes\": []}, json_file)\n",
    "    formatted_notes = []\n",
    "\n",
    "# Iterate over sample dataset.\n",
    "for i in tqdm(range(0, len(df_masked)), desc=\"Note evaluation progress\"):\n",
    "\n",
    "    # Read note and masked_note to sample masked dataset.\n",
    "    note = df_masked['note'][i]\n",
    "    masked_note = df_masked['summary'][i]\n",
    "    sensitive_entity_note = ast.literal_eval(df_masked['sensitive_entity_note'][i])\n",
    "    index = df_masked['index'][i]\n",
    "\n",
    "    # ALID\n",
    "    ALID = compute_average_levenshtein_index_of_dissimilarity(masked_note, sensitive_entity_note)\n",
    "\n",
    "    # LR\n",
    "    LR = compute_LR_ths(masked_note, sensitive_entity_note)\n",
    "\n",
    "    # LRQI\n",
    "    LRQI = compute_LRQI_ths(masked_note, sensitive_entity_note, df['summary'][index])\n",
    "\n",
    "    # JSC\n",
    "    JSC = compute_jaccard_similarity_coefficient(note, masked_note)\n",
    "\n",
    "    # NSDCG\n",
    "    NSDCG = compute_normalized_soft_discounted_cumulative_gain(note, masked_note)\n",
    "\n",
    "    # Process output.\n",
    "    metrics = process_metrics(id_note=index, ALID=ALID, LR=LR, LRQI=LRQI, JSC=JSC, NSDCG=NSDCG)\n",
    "    formatted_notes.append(metrics)\n",
    "\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump({\"notes\": formatted_notes}, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T19:22:39.272710Z",
     "start_time": "2025-08-28T19:22:39.246386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Masking: {'ALID': 49.72429, 'LR': 87.18522, 'LRQI': 83.42097428958051, 'JSC': 4.40442, 'NSDCG': 24.79625}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = './test/masked_note_eval.json'\n",
    "\n",
    "# Read json file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "notes = data[\"notes\"]\n",
    "\n",
    "metrics = [\"ALID\", \"LR\", \"LRQI\", \"JSC\", \"NSDCG\"]\n",
    "\n",
    "averages_mixed = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    if metric == \"LRQI\":\n",
    "        # Average only on values != -1\n",
    "        values = [entry[metric] for entry in notes if entry[\"LRQI\"] != -1]\n",
    "    else:\n",
    "        # Average on all entries\n",
    "        values = [entry[metric] for entry in notes]\n",
    "    averages_mixed[metric] = sum(values) / len(values) if values else None\n",
    "\n",
    "print('Data Masking:', averages_mixed)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
