{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ax3t641WR_5"
   },
   "source": [
    "### **Gemini 2.5 Pro Data Generalization Evaluation (summary)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ZSpfjx701H-",
    "outputId": "04ced276-3c54-466a-b9c3-146f2ee9ad53"
   },
   "outputs": [],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hwey6TGLCC8N",
    "outputId": "f1ff3cc6-0a39-44dc-d3a8-36bd439a405e"
   },
   "outputs": [],
   "source": [
    "%pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:31:51.122571Z",
     "start_time": "2025-08-28T16:31:50.077444Z"
    },
    "id": "_QZktNg4ayNd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8HQzX5JPdfV"
   },
   "source": [
    "#### Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:32:01.219117Z",
     "start_time": "2025-08-28T16:31:51.138088Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rafau\\Documents\\PythonProjects\\DataSanitization\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"hf://datasets/AGBonnet/augmented-clinical-notes/augmented_notes_30K.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:32:04.002717Z",
     "start_time": "2025-08-28T16:32:03.877103Z"
    },
    "id": "Uz4QGKmYPZpf"
   },
   "outputs": [],
   "source": [
    "df_general = pd.read_csv(\"../datasets/summaries/summaries_generalization_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:32:05.556141Z",
     "start_time": "2025-08-28T16:32:05.534480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>note</th>\n",
       "      <th>sensitive_entity_note</th>\n",
       "      <th>full_note</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9728</td>\n",
       "      <td>The patient was a 65-year-old male who present...</td>\n",
       "      <td>{'two mitral valve replacement operations', 'p...</td>\n",
       "      <td>The patient was a 65-year-old male who present...</td>\n",
       "      <td>The patient was an adult who presented with a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26930</td>\n",
       "      <td>A 55-year-old man, a known hypertensive and as...</td>\n",
       "      <td>{'mild breathlessness while sitting due to the...</td>\n",
       "      <td>A 55-year-old man, a known hypertensive and as...</td>\n",
       "      <td>An adult patient, a known hypertensive and ast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29415</td>\n",
       "      <td>A 38-year-old woman G6P3023 at 24 weeks gestat...</td>\n",
       "      <td>{'intraoperative urology consultation', 'Magne...</td>\n",
       "      <td>A 38-year-old woman (G6P3023) at 24 weeks gest...</td>\n",
       "      <td>An adult patient at a gestational age presente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7353</td>\n",
       "      <td>A 57-year-old male patient was transferred to ...</td>\n",
       "      <td>{'biceps tendon was found entrapped posterolat...</td>\n",
       "      <td>A 57-year-old male patient was transferred to ...</td>\n",
       "      <td>An adult patient was transferred to the emerge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9633</td>\n",
       "      <td>A 21 year old female patient reported with a c...</td>\n",
       "      <td>{'convulsions', '21 year old', 'microdontia', ...</td>\n",
       "      <td>A 21 year old female patient reported with a c...</td>\n",
       "      <td>An adult patient reported with a complaint of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>22296</td>\n",
       "      <td>The present case report is about a 21-year-old...</td>\n",
       "      <td>{\"left breast's\", 'at 1.5 years old', 'superio...</td>\n",
       "      <td>The present case report is about a 21-year-old...</td>\n",
       "      <td>The present case report is about an adult pati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2864</td>\n",
       "      <td>A 75-year-old male patient visited the hospita...</td>\n",
       "      <td>{'speech disturbance or dyspnea', 'excessive o...</td>\n",
       "      <td>A 75-year-old male patient visited the hospita...</td>\n",
       "      <td>An elderly patient visited the hospital compla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>18477</td>\n",
       "      <td>A 72-year-old man with a history of anterosept...</td>\n",
       "      <td>{'18 months earlier', 'After discharge', '86/5...</td>\n",
       "      <td>A 72-year-old man with a history of anterosept...</td>\n",
       "      <td>An elderly patient with a history of anterosep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>21552</td>\n",
       "      <td>The present case report is about a 65-year-old...</td>\n",
       "      <td>{'pus discharge', 'chewing tobacco 8-10 times ...</td>\n",
       "      <td>The present case report is about a 65-year-old...</td>\n",
       "      <td>The present case report is about an adult pati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>14697</td>\n",
       "      <td>An 11-year-old girl was referred to the cardio...</td>\n",
       "      <td>{'sudden onset', 'blood lipid profile', '11-ye...</td>\n",
       "      <td>An 11-year-old girl was referred to the cardio...</td>\n",
       "      <td>An adolescent patient was referred to the card...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                               note  \\\n",
       "0     9728  The patient was a 65-year-old male who present...   \n",
       "1    26930  A 55-year-old man, a known hypertensive and as...   \n",
       "2    29415  A 38-year-old woman G6P3023 at 24 weeks gestat...   \n",
       "3     7353  A 57-year-old male patient was transferred to ...   \n",
       "4     9633  A 21 year old female patient reported with a c...   \n",
       "..     ...                                                ...   \n",
       "995  22296  The present case report is about a 21-year-old...   \n",
       "996   2864  A 75-year-old male patient visited the hospita...   \n",
       "997  18477  A 72-year-old man with a history of anterosept...   \n",
       "998  21552  The present case report is about a 65-year-old...   \n",
       "999  14697  An 11-year-old girl was referred to the cardio...   \n",
       "\n",
       "                                 sensitive_entity_note  \\\n",
       "0    {'two mitral valve replacement operations', 'p...   \n",
       "1    {'mild breathlessness while sitting due to the...   \n",
       "2    {'intraoperative urology consultation', 'Magne...   \n",
       "3    {'biceps tendon was found entrapped posterolat...   \n",
       "4    {'convulsions', '21 year old', 'microdontia', ...   \n",
       "..                                                 ...   \n",
       "995  {\"left breast's\", 'at 1.5 years old', 'superio...   \n",
       "996  {'speech disturbance or dyspnea', 'excessive o...   \n",
       "997  {'18 months earlier', 'After discharge', '86/5...   \n",
       "998  {'pus discharge', 'chewing tobacco 8-10 times ...   \n",
       "999  {'sudden onset', 'blood lipid profile', '11-ye...   \n",
       "\n",
       "                                             full_note  \\\n",
       "0    The patient was a 65-year-old male who present...   \n",
       "1    A 55-year-old man, a known hypertensive and as...   \n",
       "2    A 38-year-old woman (G6P3023) at 24 weeks gest...   \n",
       "3    A 57-year-old male patient was transferred to ...   \n",
       "4    A 21 year old female patient reported with a c...   \n",
       "..                                                 ...   \n",
       "995  The present case report is about a 21-year-old...   \n",
       "996  A 75-year-old male patient visited the hospita...   \n",
       "997  A 72-year-old man with a history of anterosept...   \n",
       "998  The present case report is about a 65-year-old...   \n",
       "999  An 11-year-old girl was referred to the cardio...   \n",
       "\n",
       "                                               summary  \n",
       "0    The patient was an adult who presented with a ...  \n",
       "1    An adult patient, a known hypertensive and ast...  \n",
       "2    An adult patient at a gestational age presente...  \n",
       "3    An adult patient was transferred to the emerge...  \n",
       "4    An adult patient reported with a complaint of ...  \n",
       "..                                                 ...  \n",
       "995  The present case report is about an adult pati...  \n",
       "996  An elderly patient visited the hospital compla...  \n",
       "997  An elderly patient with a history of anterosep...  \n",
       "998  The present case report is about an adult pati...  \n",
       "999  An adolescent patient was referred to the card...  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:32:09.264224Z",
     "start_time": "2025-08-28T16:32:09.259252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10 years',\n",
       " '65-year-old',\n",
       " 'Gore VIABAHN 6.0 × 50 mm',\n",
       " 'The posterior tibial artery was also occluded',\n",
       " 'ankle-brachial index of 1.0',\n",
       " 'at the level of the origin of the posterior tibial artery',\n",
       " 'bacterial endocarditis',\n",
       " 'intestinal tumor',\n",
       " 'male',\n",
       " 'one varicose veins surgery',\n",
       " 'posterior region of the proximal third of the left leg',\n",
       " 'posterior surface of the proximal third of his left leg',\n",
       " 'posterior tibial artery pulse was absent',\n",
       " 'posterior tibial artery pulse was palpable in the right lower limb',\n",
       " 'saccular dilatation in the tibioperoneal trunk with a 4.4 cm diameter',\n",
       " 'two abdominal operations',\n",
       " 'two mitral valve replacement operations'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(df_general['sensitive_entity_note'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEVFTfmZUBUL"
   },
   "source": [
    "### **Average Levenshtein Index of Dissimilarity (ALID)**\n",
    "<br>\n",
    "\n",
    "Compute LSI for each sensitive entity of observation:\n",
    "$$\n",
    "LSI = \\max_{j=1}^{L-e} LR_a(se_i, w_j)\n",
    "$$\n",
    "\n",
    "Next compute the average of each LSI calculated before:\n",
    "$$\n",
    "ALID = (1 - \\langle S \\rangle) \\times 100\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:32:12.223816Z",
     "start_time": "2025-08-28T16:32:12.217972Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXmS-pUrC8PM",
    "outputId": "6bf6ab4d-8abf-4a3c-cb01-426cb0efecf0"
   },
   "outputs": [],
   "source": [
    "def LRa(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    Levenshtein Ratio:\n",
    "    LRa(a, b) = 1 - LD(a, b) / max(len(a), len(b))\n",
    "    \"\"\"\n",
    "    len_a = len(a)\n",
    "    len_b = len(b)\n",
    "    if max(len_a, len_b) == 0:\n",
    "        return 1.0  # both strings are empty\n",
    "    ld = Levenshtein.distance(a, b)\n",
    "    return 1 - (ld / max(len_a, len_b))\n",
    "\n",
    "def compute_lsi(entity: str, text_anon: str) -> float:\n",
    "    e_len = len(entity)\n",
    "    max_lra = 0.0\n",
    "    for i in range(len(text_anon) - e_len + 1):\n",
    "        window = text_anon[i:i + e_len]\n",
    "        lra = LRa(entity, window)\n",
    "        max_lra = max(max_lra, lra)\n",
    "    return max_lra\n",
    "\n",
    "def compute_average_levenshtein_index_of_dissimilarity(anonym_note: str, sensitive_entity_note: list) -> float:\n",
    "    lsi_values = [compute_lsi(ent, anonym_note) for ent in sensitive_entity_note]\n",
    "    average_lsi = np.mean(lsi_values)\n",
    "    ALID = (1 - average_lsi) * 100\n",
    "    return ALID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:32:14.688989Z",
     "start_time": "2025-08-28T16:32:14.637465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALID: 32.59%\n"
     ]
    }
   ],
   "source": [
    "alid = compute_average_levenshtein_index_of_dissimilarity(df_general['summary'][0], ast.literal_eval(df_general['sensitive_entity_note'][0]))\n",
    "print(f\"ALID: {alid:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3suaj0HUIxv"
   },
   "source": [
    "### **Levenshtein Recall (LR)**\n",
    "\n",
    "<br>\n",
    "To calculate LR, each LSI in S is compared to a selected similarity threshold, $ths$, set to 0.85:\n",
    "\n",
    "$$\n",
    "LR@ths = \\frac{ \\sum_{i=1}^{l} (S_i < ths) }{l} \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:32:17.056674Z",
     "start_time": "2025-08-28T16:32:17.050711Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b4K6-AEQFUA",
    "outputId": "05677b87-1490-4a46-ad32-98bc7c8cdcaf"
   },
   "outputs": [],
   "source": [
    "def compute_LR_ths(anonym_note: str, sensitive_entity_note: list, ths=0.85) -> float:\n",
    "    LSI_scores = []\n",
    "    for entity in sensitive_entity_note:\n",
    "        LSI_scores.append(compute_lsi(entity, anonym_note))\n",
    "\n",
    "    if len(sensitive_entity_note) == 0:\n",
    "        return -1\n",
    "\n",
    "    # Compute recall\n",
    "    num_deidentified = sum(1 for s in LSI_scores if s < ths)\n",
    "    recall = (num_deidentified / len(sensitive_entity_note)) * 100\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:32:45.013714Z",
     "start_time": "2025-08-28T16:32:44.971817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Recall: 52.94%\n"
     ]
    }
   ],
   "source": [
    "lr_score = compute_LR_ths(df_general['summary'][0], ast.literal_eval(df_general['sensitive_entity_note'][0]))\n",
    "print(f\"Levenshtein Recall: {lr_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrpd13nOWG0q"
   },
   "source": [
    "### **Levenshtein Recall Quasi Identifier (LRQI)**\n",
    "\n",
    "The LRQI is calculated similarly to the LR but only considering quasi-identifiers.\n",
    "\n",
    "$$\n",
    "\\text{LRQI}@ths = \\frac{ \\sum_{k=1}^{l_{qi}} (S_k < ths) }{l_{qi}} \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:32:49.378829Z",
     "start_time": "2025-08-28T16:32:49.361607Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxfhW2ftWGik",
    "outputId": "71a794e6-a3d7-4a98-9ce5-42a862de0bcb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import difflib\n",
    "\n",
    "def compute_LRQI_ths(anonym_note: str, sensitive_entity_note: list, json_summary: str, ths=0.85) -> float:\n",
    "\n",
    "    def extract_qi_values_from_json(json_str: str) -> list:\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            return []\n",
    "\n",
    "        qi_values = []\n",
    "\n",
    "        # Extract patient information\n",
    "        pi = data.get(\"patient information\", {})\n",
    "        for key in [\"age\", \"sex\", \"ethnicity\", \"occupation\", \"socio economic context\"]:\n",
    "            v = pi.get(key)\n",
    "            if v and v.lower() != \"none\":\n",
    "                qi_values.append(v)\n",
    "\n",
    "        # Extract patient medical history\n",
    "        pmh = data.get(\"patient medical history\", {})\n",
    "\n",
    "        if isinstance(pmh, dict):\n",
    "\n",
    "            # Physiological context\n",
    "            phsy = pmh.get(\"physiological context\")\n",
    "            if phsy is not None:\n",
    "                if isinstance(phsy, str):\n",
    "                    phsy = phsy.split(\",\")\n",
    "                elif isinstance(phsy, dict):\n",
    "                    phsy = list(phsy.values())\n",
    "                else:\n",
    "                    phsy = []\n",
    "                for p in phsy:\n",
    "                    qi_values.append(p)\n",
    "\n",
    "            # Psychological context\n",
    "            psych = pmh.get(\"psychological context\")\n",
    "            if psych is not None:\n",
    "                if isinstance(psych, str):\n",
    "                    psych = psych.split(\",\")\n",
    "                elif isinstance(psych, dict):\n",
    "                    psych = list(psych.values())\n",
    "                else:\n",
    "                    psych = []\n",
    "                for p in psych:\n",
    "                    qi_values.append(p)\n",
    "\n",
    "        elif isinstance(pmh, str) and pmh.lower() != \"none\":\n",
    "            qi_values.append(pmh)\n",
    "\n",
    "        # Extract symptoms\n",
    "        for s in data.get(\"symptoms\", []):\n",
    "            if s is not None and isinstance(s, dict):\n",
    "                if s.get(\"time\") and s[\"time\"].lower() != \"none\":\n",
    "                    qi_values.append(s[\"time\"])\n",
    "                if s.get(\"location\") and s[\"location\"].lower() != \"none\":\n",
    "                    qi_values.append(s[\"location\"])\n",
    "                if s.get(\"name of symptom\") and s[\"name of symptom\"].lower() != \"none\":\n",
    "                    for symptom in s.get(\"name of symptom\").split(\",\"):\n",
    "                        qi_values.append(symptom)\n",
    "\n",
    "        # Extract treatments\n",
    "        for t in data.get(\"treatments\", []):\n",
    "            if t is not None and isinstance(t, dict):\n",
    "                if t.get(\"name\") and t[\"name\"].lower() != \"none\":\n",
    "                    qi_values.append(t[\"name\"])\n",
    "                if t.get(\"dosage\") and t[\"dosage\"].lower() != \"none\":\n",
    "                    qi_values.append(t[\"dosage\"])\n",
    "                if t.get(\"reaction to treatment\") and t[\"reaction to treatment\"].lower() != \"none\":\n",
    "                    for reaction in t.get(\"reaction to treatment\").split(\",\"):\n",
    "                        qi_values.append(reaction)\n",
    "                if t.get(\"time\") and t[\"time\"].lower() != \"none\":\n",
    "                    qi_values.append(t[\"time\"])\n",
    "        return qi_values\n",
    "\n",
    "\n",
    "    def filter_qi_entities(sensitive_entities, qi_values, threshold=0.5):\n",
    "        filtered = []\n",
    "        se_lower = [e.lower() for e in sensitive_entities]\n",
    "\n",
    "        for qi in qi_values:\n",
    "            qi_low = qi.lower()\n",
    "            for ent in se_lower:\n",
    "                if qi_low in ent or ent in qi_low:\n",
    "                    filtered.append(ent)\n",
    "\n",
    "        for qi in qi_values:\n",
    "            qi_low = qi.lower()\n",
    "            for se in sensitive_entities:\n",
    "                se_low = se.lower()\n",
    "                ratio = difflib.SequenceMatcher(None, qi_low, se_low).ratio()\n",
    "                if ratio >= threshold:\n",
    "                    filtered.append(se)\n",
    "\n",
    "        return list(set(filtered))\n",
    "\n",
    "    qi_vals = extract_qi_values_from_json(json_summary)\n",
    "\n",
    "    if len(qi_vals) == 0: return -1\n",
    "\n",
    "    qi_entities = filter_qi_entities(sensitive_entity_note, qi_vals)\n",
    "\n",
    "    return compute_LR_ths(anonym_note, qi_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:32:57.186084Z",
     "start_time": "2025-08-28T16:32:57.137130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Recall Quasi Identifiers: 61.54%\n"
     ]
    }
   ],
   "source": [
    "lr_score = compute_LRQI_ths(anonym_note=df_general['summary'][0], sensitive_entity_note=ast.literal_eval(df_general['sensitive_entity_note'][0]),\n",
    "                            json_summary=df['summary'][df_general['index'][0]])\n",
    "print(f\"Levenshtein Recall Quasi Identifiers: {lr_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mLErJpWUO5R"
   },
   "source": [
    "### **Jaccard Similarity Coefficient (JSC)**\n",
    "\n",
    "Let C11 be the number of classes where both representations have a value of 1 and C01 + C10 be the number of classes where the representations have different values:\n",
    "\n",
    "$$\n",
    "\\text{JSC@thb} = \\frac{C_{11}}{C_{11} + C_{01} + C_{10}} \\times 100\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:33:38.041421Z",
     "start_time": "2025-08-28T16:33:24.442506Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLh3mM4QC1rE",
    "outputId": "53456830-b761-4660-f09b-e7e84c9aee70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AkshatSurolia/ICD-10-Code-Prediction\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"AkshatSurolia/ICD-10-Code-Prediction\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "config = model.config\n",
    "\n",
    "def chunk_text_ids(text: str, max_tokens=512):\n",
    "    inputs = tokenizer(text, add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids']\n",
    "    max_len = max_tokens - 2  # reserved for special tokens\n",
    "    chunks = [input_ids[i:i+max_len] for i in range(0, len(input_ids), max_len)]\n",
    "    return chunks\n",
    "\n",
    "def aggregate_logits_from_chunks(text) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    chunks = chunk_text_ids(text, max_tokens=512)\n",
    "    total_logits = None\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # add CLS e SEP token id\n",
    "        input_ids = [tokenizer.cls_token_id] + chunk + [tokenizer.sep_token_id]\n",
    "        input_tensor = torch.tensor([input_ids])  # batch size 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor).logits.squeeze(0)  # [num_classes]\n",
    "\n",
    "        if total_logits is None:\n",
    "            total_logits = logits\n",
    "        else:\n",
    "            total_logits += logits  # sum logits\n",
    "\n",
    "    return total_logits\n",
    "\n",
    "def compute_jaccard_similarity_coefficient(original_note: str, anonym_note: str, thb: float = 0.0005) -> float:\n",
    "\n",
    "    final_logits_original_note = aggregate_logits_from_chunks(original_note)\n",
    "    final_logits_masked_note = aggregate_logits_from_chunks(anonym_note)\n",
    "\n",
    "    probs_original_note = F.softmax(final_logits_original_note, dim=0).squeeze()\n",
    "    probs_anonym_note = F.softmax(final_logits_masked_note, dim=0).squeeze()\n",
    "\n",
    "    # list of indexes\n",
    "    # results = final_logits_original_note.detach().cpu().numpy()[0].argsort()[::-1][:10]\n",
    "    # [config.id2label[ids] for ids in results]\n",
    "\n",
    "    predicted_original_note = set(torch.where(probs_original_note > thb)[0].tolist())\n",
    "    predicted_masked_note = set(torch.where(probs_anonym_note > thb)[0].tolist())\n",
    "\n",
    "    C11 = len(predicted_original_note & predicted_masked_note)\n",
    "    C01 = len(predicted_masked_note - predicted_original_note)\n",
    "    C10 = len(predicted_original_note - predicted_masked_note)\n",
    "\n",
    "    denom = C11 + C01 + C10\n",
    "\n",
    "    if denom == 0: return 100.0  # noone classed\n",
    "\n",
    "    return (C11 / denom) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:33:41.066251Z",
     "start_time": "2025-08-28T16:33:39.927654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSC Score: 33.87%\n"
     ]
    }
   ],
   "source": [
    "jsc_score = compute_jaccard_similarity_coefficient(df_general['note'][0], df_general['summary'][0])\n",
    "print(f\"JSC Score: {jsc_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7l2OtVdOUUfl"
   },
   "source": [
    "### **Normalized Soft Discounted Cumulative Gain (NSDCG@K)**\n",
    "\n",
    "**SDCG@K (Soft Discounted Cumulative Gain)** is a metric used to evaluate the ranking quality of the logits produced by a model on anonymized notes compared to the original ones.\n",
    "\n",
    "$$\n",
    "\\text{SDCG@K} = \\sum_{i=1}^{K} sdi \\cdot reli\n",
    "$$\n",
    "\n",
    "- $sd_i$: soft discount factor at position *i*\n",
    "- $rel_i$: relevance of the item at position *i*\n",
    "\n",
    "Given a vector of **sorted (descending) logits** \\( s \\), the soft discount at position *i* is computed using the **softmax** function:\n",
    "\n",
    "$$\n",
    "sdi = \\frac{e^{s_i}}{\\sum_{j=1}^{N} e^{s_j}}\n",
    "$$\n",
    "\n",
    "Where $N$ is the number of ICD-10 classes.\n",
    "\n",
    "The relevance of an item at position *i* in the original logits $z$ (i.e., logits from the original note ranked according to the anonymized note) is defined as:\n",
    "\n",
    "$$\n",
    "reli = e^{z_i}\n",
    "$$\n",
    "\n",
    "**NSDCG@K (Normalized SDCG)** is obtained by dividing the SDCG of the anonymized note by the SDCG of the ideal/original note and expressing the result as a percentage:\n",
    "\n",
    "$$\n",
    "\\text{NSDCG@K} = \\frac{\\text{SDCG@K}}{\\text{ISDCG@K}} \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:33:43.852853Z",
     "start_time": "2025-08-28T16:33:43.845853Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RPMYsFy6ohi",
    "outputId": "b14a8ea5-9837-4075-829e-0eb4059adc26"
   },
   "outputs": [],
   "source": [
    "def compute_normalized_soft_discounted_cumulative_gain(original_note: str, anonym_note: str, k=10) -> float:\n",
    "\n",
    "    logits_orig = aggregate_logits_from_chunks(original_note)\n",
    "    logits_anon = aggregate_logits_from_chunks(anonym_note)\n",
    "\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    # top-K indexes from logit of masked text\n",
    "    top_k_values, top_k_indices_anon = torch.topk(logits_anon, k)\n",
    "\n",
    "    # relevance: original logit ordered via top-K of masked text\n",
    "    relevances = np.exp(logits_orig[top_k_indices_anon])\n",
    "\n",
    "    # softmax discount of original logit ordered decreasing\n",
    "    sorted_logits_orig = np.sort(logits_orig)[::-1]\n",
    "    softmax_discounts = softmax(sorted_logits_orig[:k])\n",
    "\n",
    "    # compute SDCG\n",
    "    sdcg = np.sum(softmax_discounts * relevances.cpu().numpy())\n",
    "\n",
    "    # compute ISDCG (ideal)\n",
    "    ideal_relevances = np.exp(sorted_logits_orig[:k])\n",
    "    ideal_discounts = softmax(sorted_logits_orig[:k])\n",
    "    isdcg = np.sum(ideal_discounts * ideal_relevances)\n",
    "\n",
    "    nsdcg = (sdcg / isdcg) * 100 if isdcg != 0 else 0\n",
    "\n",
    "    return nsdcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:33:49.192782Z",
     "start_time": "2025-08-28T16:33:48.273197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSDCG@K Score: 78.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rafau\\AppData\\Local\\Temp\\ipykernel_57480\\2573721112.py:14: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  relevances = np.exp(logits_orig[top_k_indices_anon])\n"
     ]
    }
   ],
   "source": [
    "nsdcg_score = compute_normalized_soft_discounted_cumulative_gain(df_general['note'][0], df_general['summary'][0])\n",
    "print(f\"NSDCG@K Score: {nsdcg_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute metrics for all records in masked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:33:52.100101Z",
     "start_time": "2025-08-28T16:33:52.095136Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process the desired output.\n",
    "def process_metrics(id_note: int, ALID: float, LR: float, LRQI: float, JSC: float, NSDCG: float) -> dict:\n",
    "\n",
    "    return {\n",
    "    \"id_note\": int(id_note),\n",
    "    \"ALID\": round(float(ALID), 2),\n",
    "    \"LR\": round(float(LR), 2),\n",
    "    \"LRQI\": round(float(LRQI), 2),\n",
    "    \"JSC\": round(float(JSC), 2),\n",
    "    \"NSDCG\": round(float(NSDCG), 2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T16:57:00.412858Z",
     "start_time": "2025-08-28T16:34:54.627827Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note evaluation progress:   0%|          | 0/507 [00:00<?, ?it/s]C:\\Users\\rafau\\AppData\\Local\\Temp\\ipykernel_57480\\2573721112.py:14: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  relevances = np.exp(logits_orig[top_k_indices_anon])\n",
      "Note evaluation progress: 100%|██████████| 507/507 [22:05<00:00,  2.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# Check if exists test folder\n",
    "if 'test' not in os.listdir():\n",
    "    os.mkdir('test')\n",
    "\n",
    "# File to save masked note evaluation.\n",
    "file_path = f'./test/gemini_generalization_note_eval.json'\n",
    "\n",
    "# Check if file exists.\n",
    "if os.path.exists(file_path):\n",
    "    # Empty file.\n",
    "    if os.stat(file_path).st_size == 0:\n",
    "        formatted_notes = []\n",
    "    else:\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            formatted_notes = data.get('notes', [])\n",
    "else:\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump({\"notes\": []}, json_file)\n",
    "    formatted_notes = []\n",
    "\n",
    "# Iterate over sample dataset.\n",
    "for i in tqdm(range(len(formatted_notes), len(df_general)), desc=\"Note evaluation progress\"):\n",
    "\n",
    "    # Read note and masked_note to sample masked dataset.\n",
    "    note = df_general['note'][i]\n",
    "    anonym_note = df_general['summary'][i]\n",
    "    sensitive_entity_note = ast.literal_eval(df_general['sensitive_entity_note'][i])\n",
    "    index = df_general['index'][i]\n",
    "\n",
    "    # ALID\n",
    "    ALID = compute_average_levenshtein_index_of_dissimilarity(anonym_note, sensitive_entity_note)\n",
    "\n",
    "    # LR\n",
    "    LR = compute_LR_ths(anonym_note, sensitive_entity_note)\n",
    "\n",
    "    # LRQI\n",
    "    LRQI = compute_LRQI_ths(anonym_note, sensitive_entity_note, df['summary'][index])\n",
    "\n",
    "    # JSC\n",
    "    JSC = compute_jaccard_similarity_coefficient(note, anonym_note)\n",
    "\n",
    "    # NSDCG\n",
    "    NSDCG = compute_normalized_soft_discounted_cumulative_gain(note, anonym_note)\n",
    "\n",
    "    # Process output.\n",
    "    metrics = process_metrics(id_note=index, ALID=ALID, LR=LR, LRQI=LRQI, JSC=JSC, NSDCG=NSDCG)\n",
    "    formatted_notes.append(metrics)\n",
    "\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump({\"notes\": formatted_notes}, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T19:20:12.046078Z",
     "start_time": "2025-08-28T19:20:11.886050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Generalization (Gemini 2.5 Pro): {'ALID': 34.23424, 'LR': 60.445879999999995, 'LRQI': 64.4045283018868, 'JSC': 28.99812, 'NSDCG': 70.27456}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = './test/gemini_generalization_note_eval.json'\n",
    "\n",
    "# Read json file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "notes = data[\"notes\"]\n",
    "\n",
    "metrics = [\"ALID\", \"LR\", \"LRQI\", \"JSC\", \"NSDCG\"]\n",
    "\n",
    "averages_mixed = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    if metric == \"LRQI\":\n",
    "        # Average only on values != -1\n",
    "        values = [entry[metric] for entry in notes if entry[\"LRQI\"] != -1]\n",
    "    else:\n",
    "        # Average on all entries\n",
    "        values = [entry[metric] for entry in notes]\n",
    "    averages_mixed[metric] = sum(values) / len(values) if values else None\n",
    "\n",
    "print('Data Generalization (Gemini 2.5 Pro):', averages_mixed)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
