{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ax3t641WR_5"
   },
   "source": [
    "### **GPT4 Data Generalization Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:17:38.697449Z",
     "start_time": "2025-08-10T10:17:32.987529Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ZSpfjx701H-",
    "outputId": "04ced276-3c54-466a-b9c3-146f2ee9ad53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (4.55.0)\n",
      "Requirement already satisfied: torch in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:17:41.383681Z",
     "start_time": "2025-08-10T10:17:38.730343Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hwey6TGLCC8N",
    "outputId": "f1ff3cc6-0a39-44dc-d3a8-36bd439a405e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (0.27.1)\n",
      "Requirement already satisfied: Levenshtein==0.27.1 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from python-Levenshtein) (0.27.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in c:\\users\\rafau\\documents\\pythonprojects\\datasanitization\\venv\\lib\\site-packages (from Levenshtein==0.27.1->python-Levenshtein) (3.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T15:48:43.404396Z",
     "start_time": "2025-08-11T15:48:42.479287Z"
    },
    "id": "_QZktNg4ayNd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8HQzX5JPdfV"
   },
   "source": [
    "#### Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:09:58.711581Z",
     "start_time": "2025-08-11T16:09:50.225845Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"hf://datasets/AGBonnet/augmented-clinical-notes/augmented_notes_30K.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T15:49:50.656595Z",
     "start_time": "2025-08-11T15:49:50.357857Z"
    },
    "id": "Uz4QGKmYPZpf"
   },
   "outputs": [],
   "source": [
    "df_anonym_sample = pd.read_csv(\"../datasets/sample/gpt4_generalization_dataset_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T13:54:59.494228Z",
     "start_time": "2025-08-10T13:54:59.441391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>note</th>\n",
       "      <th>anonym_note</th>\n",
       "      <th>sensitive_entity_note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2308</td>\n",
       "      <td>A 30-year-old man sustained an anterior disloc...</td>\n",
       "      <td>An adult individual sustained an anterior disl...</td>\n",
       "      <td>{'coracoid osteotomy was performed that enable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22404</td>\n",
       "      <td>A 64-year-old Japanese man consulted our hospi...</td>\n",
       "      <td>An adult patient of East Asian origin presente...</td>\n",
       "      <td>{'hypertension', 'transverse mesentery immedia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23397</td>\n",
       "      <td>A 61 year old man presented to emergency room ...</td>\n",
       "      <td>An adult individual presented to the emergency...</td>\n",
       "      <td>{'dysuria and intermittent hematuria', 'urgenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25058</td>\n",
       "      <td>A 33-year-old woman presented at the emergency...</td>\n",
       "      <td>An adult patient presented at the emergency de...</td>\n",
       "      <td>{'MRI of the spine demonstrated a homogeneous ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2664</td>\n",
       "      <td>A healthy 36-year-old woman with no past medic...</td>\n",
       "      <td>A healthy adult individual with no significant...</td>\n",
       "      <td>{'volume change', 'splenomegaly', 'blood bioch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>6778</td>\n",
       "      <td>A 39-year-old woman was scheduled to undergo s...</td>\n",
       "      <td>An adult patient was scheduled to undergo spli...</td>\n",
       "      <td>{'subclavian and axillary veins on the anterio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>25284</td>\n",
       "      <td>An 18-year-old male high school wrestler with ...</td>\n",
       "      <td>A young adult individual with no significant p...</td>\n",
       "      <td>{'methicillin-susceptible Staphylococcus aureu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>18355</td>\n",
       "      <td>A 61-year-old African American female was foun...</td>\n",
       "      <td>A person was found unconscious on the road aft...</td>\n",
       "      <td>{'female', '61-year-old', 'angiogram for angio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>27684</td>\n",
       "      <td>73-year-old female with history of hypertensio...</td>\n",
       "      <td>An older adult with a history of chronic cardi...</td>\n",
       "      <td>{'dry oral mucous membranes', 'more than 50 wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>4110</td>\n",
       "      <td>A 52-year-old woman presented with pain and ac...</td>\n",
       "      <td>An adult patient presented with pain and reduc...</td>\n",
       "      <td>{'woman', 'shoulder Fig.', 'alkaline phosphata...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               note  \\\n",
       "0      2308  A 30-year-old man sustained an anterior disloc...   \n",
       "1     22404  A 64-year-old Japanese man consulted our hospi...   \n",
       "2     23397  A 61 year old man presented to emergency room ...   \n",
       "3     25058  A 33-year-old woman presented at the emergency...   \n",
       "4      2664  A healthy 36-year-old woman with no past medic...   \n",
       "...     ...                                                ...   \n",
       "4995   6778  A 39-year-old woman was scheduled to undergo s...   \n",
       "4996  25284  An 18-year-old male high school wrestler with ...   \n",
       "4997  18355  A 61-year-old African American female was foun...   \n",
       "4998  27684  73-year-old female with history of hypertensio...   \n",
       "4999   4110  A 52-year-old woman presented with pain and ac...   \n",
       "\n",
       "                                            anonym_note  \\\n",
       "0     An adult individual sustained an anterior disl...   \n",
       "1     An adult patient of East Asian origin presente...   \n",
       "2     An adult individual presented to the emergency...   \n",
       "3     An adult patient presented at the emergency de...   \n",
       "4     A healthy adult individual with no significant...   \n",
       "...                                                 ...   \n",
       "4995  An adult patient was scheduled to undergo spli...   \n",
       "4996  A young adult individual with no significant p...   \n",
       "4997  A person was found unconscious on the road aft...   \n",
       "4998  An older adult with a history of chronic cardi...   \n",
       "4999  An adult patient presented with pain and reduc...   \n",
       "\n",
       "                                  sensitive_entity_note  \n",
       "0     {'coracoid osteotomy was performed that enable...  \n",
       "1     {'hypertension', 'transverse mesentery immedia...  \n",
       "2     {'dysuria and intermittent hematuria', 'urgenc...  \n",
       "3     {'MRI of the spine demonstrated a homogeneous ...  \n",
       "4     {'volume change', 'splenomegaly', 'blood bioch...  \n",
       "...                                                 ...  \n",
       "4995  {'subclavian and axillary veins on the anterio...  \n",
       "4996  {'methicillin-susceptible Staphylococcus aureu...  \n",
       "4997  {'female', '61-year-old', 'angiogram for angio...  \n",
       "4998  {'dry oral mucous membranes', 'more than 50 wh...  \n",
       "4999  {'woman', 'shoulder Fig.', 'alkaline phosphata...  \n",
       "\n",
       "[5000 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anonym_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:17:53.321228Z",
     "start_time": "2025-08-10T10:17:53.314337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'30-year-old man',\n",
       " 'COVID-19 status was confirmed to be negative',\n",
       " 'L4 vertebral level of internal rotation',\n",
       " 'and 50° of forward flexion',\n",
       " 'computed tomography imaging did not reveal any glenoid bone loss',\n",
       " 'coracoid osteotomy was performed that enabled the conjoint tendon to be retracted further medially and allowed the exposure of the LT and the attached subscapularis',\n",
       " 'delay of 3 months',\n",
       " 'full personal protective equipment precautions as was the protocol for all surgeries',\n",
       " 'functional ability to use the injured limb was limited because he had severe pain and a restricted shoulder range of motion of −10° of external rotation',\n",
       " 'lesser tuberosity LT was not visible even with a medial retraction of the conjoint tendon because of severe medial displacement of the proximal humerus',\n",
       " 'no X-ray imaging or further management',\n",
       " 'open reduction through a deltopectoral approach',\n",
       " 'operated on',\n",
       " 'primary physicians in primary health services',\n",
       " 'reduction of the GT was not possible without debulking the lateral part of the proximal metaphysis of the proximal humerus as ossification and fibrosis restricted the anterior reduction of the GT',\n",
       " 'restricted use of the affected limb',\n",
       " 'sling',\n",
       " 'symptoms of severe pain with a visual analog scale VAS score of 8 points out of a maximum of 10 points',\n",
       " 'the entire subscapularis had to be detached later repaired',\n",
       " 'the patient',\n",
       " 'town located 150 kilometers from the main city',\n",
       " 'unable to travel because of the lockdown restrictions'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(df_anonym_sample['sensitive_entity_note'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEVFTfmZUBUL"
   },
   "source": [
    "### **Average Levenshtein Index of Dissimilarity (ALID)**\n",
    "<br>\n",
    "\n",
    "Compute LSI for each sensitive entity of observation:\n",
    "$$\n",
    "LSI = \\max_{j=1}^{L-e} LR_a(se_i, w_j)\n",
    "$$\n",
    "\n",
    "Next compute the average of each LSI calculated before:\n",
    "$$\n",
    "ALID = (1 - \\langle S \\rangle) \\times 100\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:05:49.700056Z",
     "start_time": "2025-08-11T16:05:49.694542Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXmS-pUrC8PM",
    "outputId": "6bf6ab4d-8abf-4a3c-cb01-426cb0efecf0"
   },
   "outputs": [],
   "source": [
    "def LRa(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    Levenshtein Ratio:\n",
    "    LRa(a, b) = 1 - LD(a, b) / max(len(a), len(b))\n",
    "    \"\"\"\n",
    "    len_a = len(a)\n",
    "    len_b = len(b)\n",
    "    if max(len_a, len_b) == 0:\n",
    "        return 1.0  # both strings are empty\n",
    "    ld = Levenshtein.distance(a, b)\n",
    "    return 1 - (ld / max(len_a, len_b))\n",
    "\n",
    "def compute_lsi(entity: str, text_anon: str) -> float:\n",
    "    e_len = len(entity)\n",
    "    max_lra = 0.0\n",
    "    for i in range(len(text_anon) - e_len + 1):\n",
    "        window = text_anon[i:i + e_len]\n",
    "        lra = LRa(entity, window)\n",
    "        max_lra = max(max_lra, lra)\n",
    "    return max_lra\n",
    "\n",
    "def compute_average_levenshtein_index_of_dissimilarity(anonym_note: str, sensitive_entity_note: list) -> float:\n",
    "    lsi_values = [compute_lsi(ent, anonym_note) for ent in sensitive_entity_note]\n",
    "    average_lsi = np.mean(lsi_values)\n",
    "    ALID = (1 - average_lsi) * 100\n",
    "    return ALID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:05:56.789558Z",
     "start_time": "2025-08-11T16:05:56.692435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALID: 50.74%\n"
     ]
    }
   ],
   "source": [
    "alid = compute_average_levenshtein_index_of_dissimilarity(df_anonym_sample['anonym_note'][0], ast.literal_eval(df_anonym_sample['sensitive_entity_note'][0]))\n",
    "print(f\"ALID: {alid:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3suaj0HUIxv"
   },
   "source": [
    "### **Levenshtein Recall (LR)**\n",
    "\n",
    "<br>\n",
    "To calculate LR, each LSI in S is compared to a selected similarity threshold, $ths$, set to 0.85:\n",
    "\n",
    "$$\n",
    "LR@ths = \\frac{ \\sum_{i=1}^{l} (S_i < ths) }{l} \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:06:00.873990Z",
     "start_time": "2025-08-11T16:06:00.869690Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b4K6-AEQFUA",
    "outputId": "05677b87-1490-4a46-ad32-98bc7c8cdcaf"
   },
   "outputs": [],
   "source": [
    "def compute_LR_ths(anonym_note: str, sensitive_entity_note: list, ths=0.85) -> float:\n",
    "    LSI_scores = []\n",
    "    for entity in sensitive_entity_note:\n",
    "        LSI_scores.append(compute_lsi(entity, anonym_note))\n",
    "\n",
    "    if len(sensitive_entity_note) == 0:\n",
    "        return -1\n",
    "\n",
    "    # Compute recall\n",
    "    num_deidentified = sum(1 for s in LSI_scores if s < ths)\n",
    "    recall = (num_deidentified / len(sensitive_entity_note)) * 100\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:06:18.021527Z",
     "start_time": "2025-08-11T16:06:17.913529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Recall: 88.10%\n"
     ]
    }
   ],
   "source": [
    "lr_score = compute_LR_ths(df_anonym_sample['anonym_note'][0], ast.literal_eval(df_anonym_sample['sensitive_entity_note'][0]))\n",
    "print(f\"Levenshtein Recall: {lr_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrpd13nOWG0q"
   },
   "source": [
    "### **Levenshtein Recall Quasi Identifier (LRQI)**\n",
    "\n",
    "The LRQI is calculated similarly to the LR but only considering quasi-identifiers.\n",
    "\n",
    "$$\n",
    "\\text{LRQI}@ths = \\frac{ \\sum_{k=1}^{l_{qi}} (S_k < ths) }{l_{qi}} \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:09:27.389603Z",
     "start_time": "2025-08-11T16:09:27.375607Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxfhW2ftWGik",
    "outputId": "71a794e6-a3d7-4a98-9ce5-42a862de0bcb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import difflib\n",
    "\n",
    "def compute_LRQI_ths(anonym_note: str, sensitive_entity_note: list, json_summary: str, ths=0.85) -> float:\n",
    "\n",
    "    def extract_qi_values_from_json(json_str: str) -> list:\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            return []\n",
    "\n",
    "        qi_values = []\n",
    "\n",
    "        # Extract patient information\n",
    "        pi = data.get(\"patient information\", {})\n",
    "        for key in [\"age\", \"sex\", \"ethnicity\", \"occupation\", \"socio economic context\"]:\n",
    "            v = pi.get(key)\n",
    "            if v and v.lower() != \"none\":\n",
    "                qi_values.append(v)\n",
    "\n",
    "        # Extract patient medical history\n",
    "        pmh = data.get(\"patient medical history\", {})\n",
    "\n",
    "        if isinstance(pmh, dict):\n",
    "\n",
    "            # Physiological context\n",
    "            phsy = pmh.get(\"physiological context\")\n",
    "            if phsy is not None:\n",
    "                if isinstance(phsy, str):\n",
    "                    phsy = phsy.split(\",\")\n",
    "                elif isinstance(phsy, dict):\n",
    "                    phsy = list(phsy.values())\n",
    "                else:\n",
    "                    phsy = []\n",
    "                for p in phsy:\n",
    "                    qi_values.append(p)\n",
    "\n",
    "            # Psychological context\n",
    "            psych = pmh.get(\"psychological context\")\n",
    "            if psych is not None:\n",
    "                if isinstance(psych, str):\n",
    "                    psych = psych.split(\",\")\n",
    "                elif isinstance(psych, dict):\n",
    "                    psych = list(psych.values())\n",
    "                else:\n",
    "                    psych = []\n",
    "                for p in psych:\n",
    "                    qi_values.append(p)\n",
    "\n",
    "        elif isinstance(pmh, str) and pmh.lower() != \"none\":\n",
    "            qi_values.append(pmh)\n",
    "\n",
    "        # Extract symptoms\n",
    "        for s in data.get(\"symptoms\", []):\n",
    "            if s is not None and isinstance(s, dict):\n",
    "                if s.get(\"time\") and s[\"time\"].lower() != \"none\":\n",
    "                    qi_values.append(s[\"time\"])\n",
    "                if s.get(\"location\") and s[\"location\"].lower() != \"none\":\n",
    "                    qi_values.append(s[\"location\"])\n",
    "                if s.get(\"name of symptom\") and s[\"name of symptom\"].lower() != \"none\":\n",
    "                    for symptom in s.get(\"name of symptom\").split(\",\"):\n",
    "                        qi_values.append(symptom)\n",
    "\n",
    "        # Extract treatments\n",
    "        for t in data.get(\"treatments\", []):\n",
    "            if t is not None and isinstance(t, dict):\n",
    "                if t.get(\"name\") and t[\"name\"].lower() != \"none\":\n",
    "                    qi_values.append(t[\"name\"])\n",
    "                if t.get(\"dosage\") and t[\"dosage\"].lower() != \"none\":\n",
    "                    qi_values.append(t[\"dosage\"])\n",
    "                if t.get(\"reaction to treatment\") and t[\"reaction to treatment\"].lower() != \"none\":\n",
    "                    for reaction in t.get(\"reaction to treatment\").split(\",\"):\n",
    "                        qi_values.append(reaction)\n",
    "                if t.get(\"time\") and t[\"time\"].lower() != \"none\":\n",
    "                    qi_values.append(t[\"time\"])\n",
    "        return qi_values\n",
    "\n",
    "\n",
    "    def filter_qi_entities(sensitive_entities, qi_values, threshold=0.5):\n",
    "        filtered = []\n",
    "        se_lower = [e.lower() for e in sensitive_entities]\n",
    "\n",
    "        for qi in qi_values:\n",
    "            qi_low = qi.lower()\n",
    "            for ent in se_lower:\n",
    "                if qi_low in ent or ent in qi_low:\n",
    "                    filtered.append(ent)\n",
    "\n",
    "        for qi in qi_values:\n",
    "            qi_low = qi.lower()\n",
    "            for se in sensitive_entities:\n",
    "                se_low = se.lower()\n",
    "                ratio = difflib.SequenceMatcher(None, qi_low, se_low).ratio()\n",
    "                if ratio >= threshold:\n",
    "                    filtered.append(se)\n",
    "\n",
    "        return list(set(filtered))\n",
    "\n",
    "    qi_vals = extract_qi_values_from_json(json_summary)\n",
    "\n",
    "    if len(qi_vals) == 0: return -1\n",
    "\n",
    "    qi_entities = filter_qi_entities(sensitive_entity_note, qi_vals)\n",
    "\n",
    "    return compute_LR_ths(anonym_note, qi_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:15:40.341328Z",
     "start_time": "2025-08-11T16:15:40.282812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Recall Quasi Identifiers: 75.00%\n"
     ]
    }
   ],
   "source": [
    "lr_score = compute_LRQI_ths(anonym_note=df_anonym_sample['anonym_note'][0], sensitive_entity_note=ast.literal_eval(df_anonym_sample['sensitive_entity_note'][0]),\n",
    "                            json_summary=df['summary'][df_anonym_sample['index'][0]])\n",
    "print(f\"Levenshtein Recall Quasi Identifiers: {lr_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mLErJpWUO5R"
   },
   "source": [
    "### **Jaccard Similarity Coefficient (JSC)**\n",
    "\n",
    "Let C11 be the number of classes where both representations have a value of 1 and C01 + C10 be the number of classes where the representations have different values:\n",
    "\n",
    "$$\n",
    "\\text{JSC@thb} = \\frac{C_{11}}{C_{11} + C_{01} + C_{10}} \\times 100\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:16:27.939563Z",
     "start_time": "2025-08-11T16:15:58.932685Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLh3mM4QC1rE",
    "outputId": "53456830-b761-4660-f09b-e7e84c9aee70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AkshatSurolia/ICD-10-Code-Prediction\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"AkshatSurolia/ICD-10-Code-Prediction\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "config = model.config\n",
    "\n",
    "def chunk_text_ids(text: str, max_tokens=512):\n",
    "    inputs = tokenizer(text, add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids']\n",
    "    max_len = max_tokens - 2  # reserved for special tokens\n",
    "    chunks = [input_ids[i:i+max_len] for i in range(0, len(input_ids), max_len)]\n",
    "    return chunks\n",
    "\n",
    "def aggregate_logits_from_chunks(text) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    chunks = chunk_text_ids(text, max_tokens=512)\n",
    "    total_logits = None\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # add CLS e SEP token id\n",
    "        input_ids = [tokenizer.cls_token_id] + chunk + [tokenizer.sep_token_id]\n",
    "        input_tensor = torch.tensor([input_ids])  # batch size 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor).logits.squeeze(0)  # [num_classes]\n",
    "\n",
    "        if total_logits is None:\n",
    "            total_logits = logits\n",
    "        else:\n",
    "            total_logits += logits  # sum logits\n",
    "\n",
    "    return total_logits\n",
    "\n",
    "def compute_jaccard_similarity_coefficient(original_note: str, anonym_note: str, thb: float = 0.0005) -> float:\n",
    "\n",
    "    final_logits_original_note = aggregate_logits_from_chunks(original_note)\n",
    "    final_logits_masked_note = aggregate_logits_from_chunks(anonym_note)\n",
    "\n",
    "    probs_original_note = F.softmax(final_logits_original_note, dim=0).squeeze()\n",
    "    probs_anonym_note = F.softmax(final_logits_masked_note, dim=0).squeeze()\n",
    "\n",
    "    # list of indexes\n",
    "    # results = final_logits_original_note.detach().cpu().numpy()[0].argsort()[::-1][:10]\n",
    "    # [config.id2label[ids] for ids in results]\n",
    "\n",
    "    predicted_original_note = set(torch.where(probs_original_note > thb)[0].tolist())\n",
    "    predicted_masked_note = set(torch.where(probs_anonym_note > thb)[0].tolist())\n",
    "\n",
    "    C11 = len(predicted_original_note & predicted_masked_note)\n",
    "    C01 = len(predicted_masked_note - predicted_original_note)\n",
    "    C10 = len(predicted_original_note - predicted_masked_note)\n",
    "\n",
    "    denom = C11 + C01 + C10\n",
    "\n",
    "    if denom == 0: return 100.0  # noone classed\n",
    "\n",
    "    return (C11 / denom) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:18:02.565807Z",
     "start_time": "2025-08-11T16:18:01.356598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSC Score: 19.35%\n"
     ]
    }
   ],
   "source": [
    "jsc_score = compute_jaccard_similarity_coefficient(df_anonym_sample['note'][0], df_anonym_sample['anonym_note'][0])\n",
    "print(f\"JSC Score: {jsc_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7l2OtVdOUUfl"
   },
   "source": [
    "### **Normalized Soft Discounted Cumulative Gain (NSDCG@K)**\n",
    "\n",
    "**SDCG@K (Soft Discounted Cumulative Gain)** is a metric used to evaluate the ranking quality of the logits produced by a model on anonymized notes compared to the original ones.\n",
    "\n",
    "$$\n",
    "\\text{SDCG@K} = \\sum_{i=1}^{K} sdi \\cdot reli\n",
    "$$\n",
    "\n",
    "- $sd_i$: soft discount factor at position *i*\n",
    "- $rel_i$: relevance of the item at position *i*\n",
    "\n",
    "Given a vector of **sorted (descending) logits** \\( s \\), the soft discount at position *i* is computed using the **softmax** function:\n",
    "\n",
    "$$\n",
    "sdi = \\frac{e^{s_i}}{\\sum_{j=1}^{N} e^{s_j}}\n",
    "$$\n",
    "\n",
    "Where $N$ is the number of ICD-10 classes.\n",
    "\n",
    "The relevance of an item at position *i* in the original logits $z$ (i.e., logits from the original note ranked according to the anonymized note) is defined as:\n",
    "\n",
    "$$\n",
    "reli = e^{z_i}\n",
    "$$\n",
    "\n",
    "**NSDCG@K (Normalized SDCG)** is obtained by dividing the SDCG of the anonymized note by the SDCG of the ideal/original note and expressing the result as a percentage:\n",
    "\n",
    "$$\n",
    "\\text{NSDCG@K} = \\frac{\\text{SDCG@K}}{\\text{ISDCG@K}} \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:26:26.848558Z",
     "start_time": "2025-08-11T16:26:26.840628Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RPMYsFy6ohi",
    "outputId": "b14a8ea5-9837-4075-829e-0eb4059adc26"
   },
   "outputs": [],
   "source": [
    "def compute_normalized_soft_discounted_cumulative_gain(original_note: str, anonym_note: str, k=10) -> float:\n",
    "\n",
    "    logits_orig = aggregate_logits_from_chunks(original_note)\n",
    "    logits_anon = aggregate_logits_from_chunks(anonym_note)\n",
    "\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    # top-K indexes from logit of masked text\n",
    "    top_k_values, top_k_indices_anon = torch.topk(logits_anon, k)\n",
    "\n",
    "    # relevance: original logit ordered via top-K of masked text\n",
    "    relevances = np.exp(logits_orig[top_k_indices_anon])\n",
    "\n",
    "    # softmax discount of original logit ordered decreasing\n",
    "    sorted_logits_orig = np.sort(logits_orig)[::-1]\n",
    "    softmax_discounts = softmax(sorted_logits_orig[:k])\n",
    "\n",
    "    # compute SDCG\n",
    "    sdcg = np.sum(softmax_discounts * relevances.cpu().numpy())\n",
    "\n",
    "    # compute ISDCG (ideal)\n",
    "    ideal_relevances = np.exp(sorted_logits_orig[:k])\n",
    "    ideal_discounts = softmax(sorted_logits_orig[:k])\n",
    "    isdcg = np.sum(ideal_discounts * ideal_relevances)\n",
    "\n",
    "    nsdcg = (sdcg / isdcg) * 100 if isdcg != 0 else 0\n",
    "\n",
    "    return nsdcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:26:47.771333Z",
     "start_time": "2025-08-11T16:26:46.855234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSDCG@K Score: 45.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rafau\\AppData\\Local\\Temp\\ipykernel_54848\\2573721112.py:14: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  relevances = np.exp(logits_orig[top_k_indices_anon])\n"
     ]
    }
   ],
   "source": [
    "nsdcg_score = compute_normalized_soft_discounted_cumulative_gain(df_anonym_sample['note'][0], df_anonym_sample['anonym_note'][0])\n",
    "print(f\"NSDCG@K Score: {nsdcg_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute metrics for all records in masked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:18:20.962237Z",
     "start_time": "2025-08-10T10:18:20.955898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process the desired output.\n",
    "def process_metrics(id_note: int, ALID: float, LR: float, LRQI: float, JSC: float, NSDCG: float) -> dict:\n",
    "\n",
    "    return {\n",
    "    \"id_note\": int(id_note),\n",
    "    \"ALID\": round(float(ALID), 2),\n",
    "    \"LR\": round(float(LR), 2),\n",
    "    \"LRQI\": round(float(LRQI), 2),\n",
    "    \"JSC\": round(float(JSC), 2),\n",
    "    \"NSDCG\": round(float(NSDCG), 2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T15:57:19.342559Z",
     "start_time": "2025-08-10T14:20:59.319807Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note evaluation progress:   0%|          | 0/2268 [00:00<?, ?it/s]C:\\Users\\rafau\\AppData\\Local\\Temp\\ipykernel_77444\\2573721112.py:14: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  relevances = np.exp(logits_orig[top_k_indices_anon])\n",
      "Note evaluation progress: 100%|██████████| 2268/2268 [1:36:20<00:00,  2.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# Check if exists test folder\n",
    "if 'test' not in os.listdir():\n",
    "    os.mkdir('test')\n",
    "\n",
    "# File to save masked note evaluation.\n",
    "file_path = f'./test/gpt4_generalization_note_eval.json'\n",
    "\n",
    "# Check if file exists.\n",
    "if os.path.exists(file_path):\n",
    "    # Empty file.\n",
    "    if os.stat(file_path).st_size == 0:\n",
    "        formatted_notes = []\n",
    "    else:\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            formatted_notes = data.get('notes', [])\n",
    "else:\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump({\"notes\": []}, json_file)\n",
    "    formatted_notes = []\n",
    "\n",
    "# Iterate over sample dataset.\n",
    "for i in tqdm(range(0, len(df_anonym_sample)), desc=\"Note evaluation progress\"):\n",
    "\n",
    "    # Read note and masked_note to sample masked dataset.\n",
    "    note = df_anonym_sample['note'][i]\n",
    "    anonym_note = df_anonym_sample['anonym_note'][i]\n",
    "    sensitive_entity_note = ast.literal_eval(df_anonym_sample['sensitive_entity_note'][i])\n",
    "    index = df_anonym_sample['index'][i]\n",
    "\n",
    "    # ALID\n",
    "    ALID = compute_average_levenshtein_index_of_dissimilarity(anonym_note, sensitive_entity_note)\n",
    "\n",
    "    # LR\n",
    "    LR = compute_LR_ths(anonym_note, sensitive_entity_note)\n",
    "\n",
    "    # LRQI\n",
    "    LRQI = compute_LRQI_ths(anonym_note, sensitive_entity_note, df['summary'][index])\n",
    "\n",
    "    # JSC\n",
    "    JSC = compute_jaccard_similarity_coefficient(note, anonym_note)\n",
    "\n",
    "    # NSDCG\n",
    "    NSDCG = compute_normalized_soft_discounted_cumulative_gain(note, anonym_note)\n",
    "\n",
    "    # Process output.\n",
    "    metrics = process_metrics(id_note=index, ALID=ALID, LR=LR, LRQI=LRQI, JSC=JSC, NSDCG=NSDCG)\n",
    "    formatted_notes.append(metrics)\n",
    "\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump({\"notes\": formatted_notes}, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:11:15.437751Z",
     "start_time": "2025-08-14T14:11:15.409751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Generalization (GPT4): {'ALID': 48.720418, 'LR': 90.97883, 'LRQI': 91.31754510661564, 'JSC': 23.598556, 'NSDCG': 62.298188}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = './test/gpt4_generalization_note_eval.json'\n",
    "\n",
    "# Read json file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "notes = data[\"notes\"]\n",
    "\n",
    "metrics = [\"ALID\", \"LR\", \"LRQI\", \"JSC\", \"NSDCG\"]\n",
    "\n",
    "averages_mixed = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    if metric == \"LRQI\":\n",
    "        # Average only on values != -1\n",
    "        values = [entry[metric] for entry in notes if entry[\"LRQI\"] != -1]\n",
    "    else:\n",
    "        # Average on all entries\n",
    "        values = [entry[metric] for entry in notes]\n",
    "    averages_mixed[metric] = sum(values) / len(values) if values else None\n",
    "\n",
    "print('Data Generalization (GPT4):', averages_mixed)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
