{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ax3t641WR_5"
   },
   "source": [
    "### **Llama3 Data Generalization Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ZSpfjx701H-",
    "outputId": "04ced276-3c54-466a-b9c3-146f2ee9ad53"
   },
   "outputs": [],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hwey6TGLCC8N",
    "outputId": "f1ff3cc6-0a39-44dc-d3a8-36bd439a405e"
   },
   "outputs": [],
   "source": [
    "%pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T09:59:14.199723Z",
     "start_time": "2025-08-13T09:59:13.141164Z"
    },
    "id": "_QZktNg4ayNd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8HQzX5JPdfV"
   },
   "source": [
    "#### Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T09:59:27.679022Z",
     "start_time": "2025-08-13T09:59:17.928133Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rafau\\Documents\\PythonProjects\\DataSanitization\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"hf://datasets/AGBonnet/augmented-clinical-notes/augmented_notes_30K.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T09:59:50.829560Z",
     "start_time": "2025-08-13T09:59:50.593683Z"
    },
    "id": "Uz4QGKmYPZpf"
   },
   "outputs": [],
   "source": [
    "df_anonym_sample = pd.read_csv(\"../datasets/sample/llama3_generalization_dataset_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T09:59:53.150903Z",
     "start_time": "2025-08-13T09:59:53.137998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>note</th>\n",
       "      <th>anonym_note</th>\n",
       "      <th>sensitive_entity_note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2308</td>\n",
       "      <td>A 30-year-old man sustained an anterior disloc...</td>\n",
       "      <td>A patient sustained an anterior dislocation of...</td>\n",
       "      <td>{'anterior dislocation of the right shoulder',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22404</td>\n",
       "      <td>A 64-year-old Japanese man consulted our hospi...</td>\n",
       "      <td>A patient consulted our hospital due to an abd...</td>\n",
       "      <td>{'left adrenal gland', '64-year-old', 'man', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23397</td>\n",
       "      <td>A 61 year old man presented to emergency room ...</td>\n",
       "      <td>A middle-aged adult presented to emergency roo...</td>\n",
       "      <td>{'hypotensive with a blood pressure BP of 78/4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25058</td>\n",
       "      <td>A 33-year-old woman presented at the emergency...</td>\n",
       "      <td>A patient presented at the emergency departmen...</td>\n",
       "      <td>{'subacute progressive bilateral leg weakness'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2664</td>\n",
       "      <td>A healthy 36-year-old woman with no past medic...</td>\n",
       "      <td>A patient presented to the clinic with a chief...</td>\n",
       "      <td>{'microscopic examination', 'thymic tissue', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>6778</td>\n",
       "      <td>A 39-year-old woman was scheduled to undergo s...</td>\n",
       "      <td>A patient was scheduled to undergo a surgical ...</td>\n",
       "      <td>{'155 cm', '85 burn index', 'femoral veins', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>25284</td>\n",
       "      <td>An 18-year-old male high school wrestler with ...</td>\n",
       "      <td>A young adult patient with no significant past...</td>\n",
       "      <td>{'X-ray', 'male', 'vancomycin IV', 'high schoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>18355</td>\n",
       "      <td>A 61-year-old African American female was foun...</td>\n",
       "      <td>A patient was found unconscious after a suspec...</td>\n",
       "      <td>{'early evening', '4 lane road', 'pelvic bleed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>27684</td>\n",
       "      <td>73-year-old female with history of hypertensio...</td>\n",
       "      <td>An elderly adult with a history of cardiovascu...</td>\n",
       "      <td>{'127 mmol/L sodium', 'atrial fibrillation', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>4110</td>\n",
       "      <td>A 52-year-old woman presented with pain and ac...</td>\n",
       "      <td>A middle-aged adult presented with pain and fu...</td>\n",
       "      <td>{'reverse total shoulder arthroplasty', 'June ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               note  \\\n",
       "0      2308  A 30-year-old man sustained an anterior disloc...   \n",
       "1     22404  A 64-year-old Japanese man consulted our hospi...   \n",
       "2     23397  A 61 year old man presented to emergency room ...   \n",
       "3     25058  A 33-year-old woman presented at the emergency...   \n",
       "4      2664  A healthy 36-year-old woman with no past medic...   \n",
       "...     ...                                                ...   \n",
       "4995   6778  A 39-year-old woman was scheduled to undergo s...   \n",
       "4996  25284  An 18-year-old male high school wrestler with ...   \n",
       "4997  18355  A 61-year-old African American female was foun...   \n",
       "4998  27684  73-year-old female with history of hypertensio...   \n",
       "4999   4110  A 52-year-old woman presented with pain and ac...   \n",
       "\n",
       "                                            anonym_note  \\\n",
       "0     A patient sustained an anterior dislocation of...   \n",
       "1     A patient consulted our hospital due to an abd...   \n",
       "2     A middle-aged adult presented to emergency roo...   \n",
       "3     A patient presented at the emergency departmen...   \n",
       "4     A patient presented to the clinic with a chief...   \n",
       "...                                                 ...   \n",
       "4995  A patient was scheduled to undergo a surgical ...   \n",
       "4996  A young adult patient with no significant past...   \n",
       "4997  A patient was found unconscious after a suspec...   \n",
       "4998  An elderly adult with a history of cardiovascu...   \n",
       "4999  A middle-aged adult presented with pain and fu...   \n",
       "\n",
       "                                  sensitive_entity_note  \n",
       "0     {'anterior dislocation of the right shoulder',...  \n",
       "1     {'left adrenal gland', '64-year-old', 'man', '...  \n",
       "2     {'hypotensive with a blood pressure BP of 78/4...  \n",
       "3     {'subacute progressive bilateral leg weakness'...  \n",
       "4     {'microscopic examination', 'thymic tissue', '...  \n",
       "...                                                 ...  \n",
       "4995  {'155 cm', '85 burn index', 'femoral veins', '...  \n",
       "4996  {'X-ray', 'male', 'vancomycin IV', 'high schoo...  \n",
       "4997  {'early evening', '4 lane road', 'pelvic bleed...  \n",
       "4998  {'127 mmol/L sodium', 'atrial fibrillation', '...  \n",
       "4999  {'reverse total shoulder arthroplasty', 'June ...  \n",
       "\n",
       "[5000 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anonym_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T10:17:53.321228Z",
     "start_time": "2025-08-10T10:17:53.314337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'30-year-old man',\n",
       " 'COVID-19 status was confirmed to be negative',\n",
       " 'L4 vertebral level of internal rotation',\n",
       " 'and 50° of forward flexion',\n",
       " 'computed tomography imaging did not reveal any glenoid bone loss',\n",
       " 'coracoid osteotomy was performed that enabled the conjoint tendon to be retracted further medially and allowed the exposure of the LT and the attached subscapularis',\n",
       " 'delay of 3 months',\n",
       " 'full personal protective equipment precautions as was the protocol for all surgeries',\n",
       " 'functional ability to use the injured limb was limited because he had severe pain and a restricted shoulder range of motion of −10° of external rotation',\n",
       " 'lesser tuberosity LT was not visible even with a medial retraction of the conjoint tendon because of severe medial displacement of the proximal humerus',\n",
       " 'no X-ray imaging or further management',\n",
       " 'open reduction through a deltopectoral approach',\n",
       " 'operated on',\n",
       " 'primary physicians in primary health services',\n",
       " 'reduction of the GT was not possible without debulking the lateral part of the proximal metaphysis of the proximal humerus as ossification and fibrosis restricted the anterior reduction of the GT',\n",
       " 'restricted use of the affected limb',\n",
       " 'sling',\n",
       " 'symptoms of severe pain with a visual analog scale VAS score of 8 points out of a maximum of 10 points',\n",
       " 'the entire subscapularis had to be detached later repaired',\n",
       " 'the patient',\n",
       " 'town located 150 kilometers from the main city',\n",
       " 'unable to travel because of the lockdown restrictions'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(df_anonym_sample['sensitive_entity_note'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEVFTfmZUBUL"
   },
   "source": [
    "### **Average Levenshtein Index of Dissimilarity (ALID)**\n",
    "<br>\n",
    "\n",
    "Compute LSI for each sensitive entity of observation:\n",
    "$$\n",
    "LSI = \\max_{j=1}^{L-e} LR_a(se_i, w_j)\n",
    "$$\n",
    "\n",
    "Next compute the average of each LSI calculated before:\n",
    "$$\n",
    "ALID = (1 - \\langle S \\rangle) \\times 100\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:00:13.928964Z",
     "start_time": "2025-08-13T10:00:13.921983Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXmS-pUrC8PM",
    "outputId": "6bf6ab4d-8abf-4a3c-cb01-426cb0efecf0"
   },
   "outputs": [],
   "source": [
    "def LRa(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    Levenshtein Ratio:\n",
    "    LRa(a, b) = 1 - LD(a, b) / max(len(a), len(b))\n",
    "    \"\"\"\n",
    "    len_a = len(a)\n",
    "    len_b = len(b)\n",
    "    if max(len_a, len_b) == 0:\n",
    "        return 1.0  # both strings are empty\n",
    "    ld = Levenshtein.distance(a, b)\n",
    "    return 1 - (ld / max(len_a, len_b))\n",
    "\n",
    "def compute_lsi(entity: str, text_anon: str) -> float:\n",
    "    e_len = len(entity)\n",
    "    max_lra = 0.0\n",
    "    for i in range(len(text_anon) - e_len + 1):\n",
    "        window = text_anon[i:i + e_len]\n",
    "        lra = LRa(entity, window)\n",
    "        max_lra = max(max_lra, lra)\n",
    "    return max_lra\n",
    "\n",
    "def compute_average_levenshtein_index_of_dissimilarity(anonym_note: str, sensitive_entity_note: list) -> float:\n",
    "    lsi_values = [compute_lsi(ent, anonym_note) for ent in sensitive_entity_note]\n",
    "    average_lsi = np.mean(lsi_values)\n",
    "    ALID = (1 - average_lsi) * 100\n",
    "    return ALID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:00:17.501124Z",
     "start_time": "2025-08-13T10:00:17.451092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALID: 51.77%\n"
     ]
    }
   ],
   "source": [
    "alid = compute_average_levenshtein_index_of_dissimilarity(df_anonym_sample['anonym_note'][0], ast.literal_eval(df_anonym_sample['sensitive_entity_note'][0]))\n",
    "print(f\"ALID: {alid:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3suaj0HUIxv"
   },
   "source": [
    "### **Levenshtein Recall (LR)**\n",
    "\n",
    "<br>\n",
    "To calculate LR, each LSI in S is compared to a selected similarity threshold, $ths$, set to 0.85:\n",
    "\n",
    "$$\n",
    "LR@ths = \\frac{ \\sum_{i=1}^{l} (S_i < ths) }{l} \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:00:33.144089Z",
     "start_time": "2025-08-13T10:00:33.139121Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b4K6-AEQFUA",
    "outputId": "05677b87-1490-4a46-ad32-98bc7c8cdcaf"
   },
   "outputs": [],
   "source": [
    "def compute_LR_ths(anonym_note: str, sensitive_entity_note: list, ths=0.85) -> float:\n",
    "    LSI_scores = []\n",
    "    for entity in sensitive_entity_note:\n",
    "        LSI_scores.append(compute_lsi(entity, anonym_note))\n",
    "\n",
    "    if len(sensitive_entity_note) == 0:\n",
    "        return -1\n",
    "\n",
    "    # Compute recall\n",
    "    num_deidentified = sum(1 for s in LSI_scores if s < ths)\n",
    "    recall = (num_deidentified / len(sensitive_entity_note)) * 100\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:00:35.355863Z",
     "start_time": "2025-08-13T10:00:35.309050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Recall: 86.67%\n"
     ]
    }
   ],
   "source": [
    "lr_score = compute_LR_ths(df_anonym_sample['anonym_note'][0], ast.literal_eval(df_anonym_sample['sensitive_entity_note'][0]))\n",
    "print(f\"Levenshtein Recall: {lr_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrpd13nOWG0q"
   },
   "source": [
    "### **Levenshtein Recall Quasi Identifier (LRQI)**\n",
    "\n",
    "The LRQI is calculated similarly to the LR but only considering quasi-identifiers.\n",
    "\n",
    "$$\n",
    "\\text{LRQI}@ths = \\frac{ \\sum_{k=1}^{l_{qi}} (S_k < ths) }{l_{qi}} \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:00:39.786507Z",
     "start_time": "2025-08-13T10:00:39.773507Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxfhW2ftWGik",
    "outputId": "71a794e6-a3d7-4a98-9ce5-42a862de0bcb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import difflib\n",
    "\n",
    "def compute_LRQI_ths(anonym_note: str, sensitive_entity_note: list, json_summary: str, ths=0.85) -> float:\n",
    "\n",
    "    def extract_qi_values_from_json(json_str: str) -> list:\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            return []\n",
    "\n",
    "        qi_values = []\n",
    "\n",
    "        # Extract patient information\n",
    "        pi = data.get(\"patient information\", {})\n",
    "        for key in [\"age\", \"sex\", \"ethnicity\", \"occupation\", \"socio economic context\"]:\n",
    "            v = pi.get(key)\n",
    "            if v and v.lower() != \"none\":\n",
    "                qi_values.append(v)\n",
    "\n",
    "        # Extract patient medical history\n",
    "        pmh = data.get(\"patient medical history\", {})\n",
    "\n",
    "        if isinstance(pmh, dict):\n",
    "\n",
    "            # Physiological context\n",
    "            phsy = pmh.get(\"physiological context\")\n",
    "            if phsy is not None:\n",
    "                if isinstance(phsy, str):\n",
    "                    phsy = phsy.split(\",\")\n",
    "                elif isinstance(phsy, dict):\n",
    "                    phsy = list(phsy.values())\n",
    "                else:\n",
    "                    phsy = []\n",
    "                for p in phsy:\n",
    "                    qi_values.append(p)\n",
    "\n",
    "            # Psychological context\n",
    "            psych = pmh.get(\"psychological context\")\n",
    "            if psych is not None:\n",
    "                if isinstance(psych, str):\n",
    "                    psych = psych.split(\",\")\n",
    "                elif isinstance(psych, dict):\n",
    "                    psych = list(psych.values())\n",
    "                else:\n",
    "                    psych = []\n",
    "                for p in psych:\n",
    "                    qi_values.append(p)\n",
    "\n",
    "        elif isinstance(pmh, str) and pmh.lower() != \"none\":\n",
    "            qi_values.append(pmh)\n",
    "\n",
    "        # Extract symptoms\n",
    "        for s in data.get(\"symptoms\", []):\n",
    "            if s is not None and isinstance(s, dict):\n",
    "                if s.get(\"time\") and s[\"time\"].lower() != \"none\":\n",
    "                    qi_values.append(s[\"time\"])\n",
    "                if s.get(\"location\") and s[\"location\"].lower() != \"none\":\n",
    "                    qi_values.append(s[\"location\"])\n",
    "                if s.get(\"name of symptom\") and s[\"name of symptom\"].lower() != \"none\":\n",
    "                    for symptom in s.get(\"name of symptom\").split(\",\"):\n",
    "                        qi_values.append(symptom)\n",
    "\n",
    "        # Extract treatments\n",
    "        for t in data.get(\"treatments\", []):\n",
    "            if t is not None and isinstance(t, dict):\n",
    "                if t.get(\"name\") and t[\"name\"].lower() != \"none\":\n",
    "                    qi_values.append(t[\"name\"])\n",
    "                if t.get(\"dosage\") and t[\"dosage\"].lower() != \"none\":\n",
    "                    qi_values.append(t[\"dosage\"])\n",
    "                if t.get(\"reaction to treatment\") and t[\"reaction to treatment\"].lower() != \"none\":\n",
    "                    for reaction in t.get(\"reaction to treatment\").split(\",\"):\n",
    "                        qi_values.append(reaction)\n",
    "                if t.get(\"time\") and t[\"time\"].lower() != \"none\":\n",
    "                    qi_values.append(t[\"time\"])\n",
    "        return qi_values\n",
    "\n",
    "\n",
    "    def filter_qi_entities(sensitive_entities, qi_values, threshold=0.5):\n",
    "        filtered = []\n",
    "        se_lower = [e.lower() for e in sensitive_entities]\n",
    "\n",
    "        for qi in qi_values:\n",
    "            qi_low = qi.lower()\n",
    "            for ent in se_lower:\n",
    "                if qi_low in ent or ent in qi_low:\n",
    "                    filtered.append(ent)\n",
    "\n",
    "        for qi in qi_values:\n",
    "            qi_low = qi.lower()\n",
    "            for se in sensitive_entities:\n",
    "                se_low = se.lower()\n",
    "                ratio = difflib.SequenceMatcher(None, qi_low, se_low).ratio()\n",
    "                if ratio >= threshold:\n",
    "                    filtered.append(se)\n",
    "\n",
    "        return list(set(filtered))\n",
    "\n",
    "    qi_vals = extract_qi_values_from_json(json_summary)\n",
    "\n",
    "    if len(qi_vals) == 0: return -1\n",
    "\n",
    "    qi_entities = filter_qi_entities(sensitive_entity_note, qi_vals)\n",
    "\n",
    "    return compute_LR_ths(anonym_note, qi_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:00:42.293175Z",
     "start_time": "2025-08-13T10:00:42.288170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Recall Quasi Identifiers: -1.00%\n"
     ]
    }
   ],
   "source": [
    "lr_score = compute_LRQI_ths(anonym_note=df_anonym_sample['anonym_note'][0], sensitive_entity_note=ast.literal_eval(df_anonym_sample['sensitive_entity_note'][0]),\n",
    "                            json_summary=df['summary'][df_anonym_sample['index'][0]])\n",
    "print(f\"Levenshtein Recall Quasi Identifiers: {lr_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mLErJpWUO5R"
   },
   "source": [
    "### **Jaccard Similarity Coefficient (JSC)**\n",
    "\n",
    "Let C11 be the number of classes where both representations have a value of 1 and C01 + C10 be the number of classes where the representations have different values:\n",
    "\n",
    "$$\n",
    "\\text{JSC@thb} = \\frac{C_{11}}{C_{11} + C_{01} + C_{10}} \\times 100\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:01:15.102486Z",
     "start_time": "2025-08-13T10:00:47.828906Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLh3mM4QC1rE",
    "outputId": "53456830-b761-4660-f09b-e7e84c9aee70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AkshatSurolia/ICD-10-Code-Prediction\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"AkshatSurolia/ICD-10-Code-Prediction\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "config = model.config\n",
    "\n",
    "def chunk_text_ids(text: str, max_tokens=512):\n",
    "    inputs = tokenizer(text, add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids']\n",
    "    max_len = max_tokens - 2  # reserved for special tokens\n",
    "    chunks = [input_ids[i:i+max_len] for i in range(0, len(input_ids), max_len)]\n",
    "    return chunks\n",
    "\n",
    "def aggregate_logits_from_chunks(text) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    chunks = chunk_text_ids(text, max_tokens=512)\n",
    "    total_logits = None\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # add CLS e SEP token id\n",
    "        input_ids = [tokenizer.cls_token_id] + chunk + [tokenizer.sep_token_id]\n",
    "        input_tensor = torch.tensor([input_ids])  # batch size 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor).logits.squeeze(0)  # [num_classes]\n",
    "\n",
    "        if total_logits is None:\n",
    "            total_logits = logits\n",
    "        else:\n",
    "            total_logits += logits  # sum logits\n",
    "\n",
    "    return total_logits\n",
    "\n",
    "def compute_jaccard_similarity_coefficient(original_note: str, anonym_note: str, thb: float = 0.0005) -> float:\n",
    "\n",
    "    final_logits_original_note = aggregate_logits_from_chunks(original_note)\n",
    "    final_logits_masked_note = aggregate_logits_from_chunks(anonym_note)\n",
    "\n",
    "    probs_original_note = F.softmax(final_logits_original_note, dim=0).squeeze()\n",
    "    probs_anonym_note = F.softmax(final_logits_masked_note, dim=0).squeeze()\n",
    "\n",
    "    # list of indexes\n",
    "    # results = final_logits_original_note.detach().cpu().numpy()[0].argsort()[::-1][:10]\n",
    "    # [config.id2label[ids] for ids in results]\n",
    "\n",
    "    predicted_original_note = set(torch.where(probs_original_note > thb)[0].tolist())\n",
    "    predicted_masked_note = set(torch.where(probs_anonym_note > thb)[0].tolist())\n",
    "\n",
    "    C11 = len(predicted_original_note & predicted_masked_note)\n",
    "    C01 = len(predicted_masked_note - predicted_original_note)\n",
    "    C10 = len(predicted_original_note - predicted_masked_note)\n",
    "\n",
    "    denom = C11 + C01 + C10\n",
    "\n",
    "    if denom == 0: return 100.0  # noone classed\n",
    "\n",
    "    return (C11 / denom) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:01:18.288127Z",
     "start_time": "2025-08-13T10:01:17.330040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSC Score: 16.00%\n"
     ]
    }
   ],
   "source": [
    "jsc_score = compute_jaccard_similarity_coefficient(df_anonym_sample['note'][0], df_anonym_sample['anonym_note'][0])\n",
    "print(f\"JSC Score: {jsc_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7l2OtVdOUUfl"
   },
   "source": [
    "### **Normalized Soft Discounted Cumulative Gain (NSDCG@K)**\n",
    "\n",
    "**SDCG@K (Soft Discounted Cumulative Gain)** is a metric used to evaluate the ranking quality of the logits produced by a model on anonymized notes compared to the original ones.\n",
    "\n",
    "$$\n",
    "\\text{SDCG@K} = \\sum_{i=1}^{K} sdi \\cdot reli\n",
    "$$\n",
    "\n",
    "- $sd_i$: soft discount factor at position *i*\n",
    "- $rel_i$: relevance of the item at position *i*\n",
    "\n",
    "Given a vector of **sorted (descending) logits** \\( s \\), the soft discount at position *i* is computed using the **softmax** function:\n",
    "\n",
    "$$\n",
    "sdi = \\frac{e^{s_i}}{\\sum_{j=1}^{N} e^{s_j}}\n",
    "$$\n",
    "\n",
    "Where $N$ is the number of ICD-10 classes.\n",
    "\n",
    "The relevance of an item at position *i* in the original logits $z$ (i.e., logits from the original note ranked according to the anonymized note) is defined as:\n",
    "\n",
    "$$\n",
    "reli = e^{z_i}\n",
    "$$\n",
    "\n",
    "**NSDCG@K (Normalized SDCG)** is obtained by dividing the SDCG of the anonymized note by the SDCG of the ideal/original note and expressing the result as a percentage:\n",
    "\n",
    "$$\n",
    "\\text{NSDCG@K} = \\frac{\\text{SDCG@K}}{\\text{ISDCG@K}} \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:01:23.233150Z",
     "start_time": "2025-08-13T10:01:23.226847Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RPMYsFy6ohi",
    "outputId": "b14a8ea5-9837-4075-829e-0eb4059adc26"
   },
   "outputs": [],
   "source": [
    "def compute_normalized_soft_discounted_cumulative_gain(original_note: str, anonym_note: str, k=10) -> float:\n",
    "\n",
    "    logits_orig = aggregate_logits_from_chunks(original_note)\n",
    "    logits_anon = aggregate_logits_from_chunks(anonym_note)\n",
    "\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    # top-K indexes from logit of masked text\n",
    "    top_k_values, top_k_indices_anon = torch.topk(logits_anon, k)\n",
    "\n",
    "    # relevance: original logit ordered via top-K of masked text\n",
    "    relevances = np.exp(logits_orig[top_k_indices_anon])\n",
    "\n",
    "    # softmax discount of original logit ordered decreasing\n",
    "    sorted_logits_orig = np.sort(logits_orig)[::-1]\n",
    "    softmax_discounts = softmax(sorted_logits_orig[:k])\n",
    "\n",
    "    # compute SDCG\n",
    "    sdcg = np.sum(softmax_discounts * relevances.cpu().numpy())\n",
    "\n",
    "    # compute ISDCG (ideal)\n",
    "    ideal_relevances = np.exp(sorted_logits_orig[:k])\n",
    "    ideal_discounts = softmax(sorted_logits_orig[:k])\n",
    "    isdcg = np.sum(ideal_discounts * ideal_relevances)\n",
    "\n",
    "    nsdcg = (sdcg / isdcg) * 100 if isdcg != 0 else 0\n",
    "\n",
    "    return nsdcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:01:26.681727Z",
     "start_time": "2025-08-13T10:01:25.911648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSDCG@K Score: 15.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rafau\\AppData\\Local\\Temp\\ipykernel_11524\\2573721112.py:14: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  relevances = np.exp(logits_orig[top_k_indices_anon])\n"
     ]
    }
   ],
   "source": [
    "nsdcg_score = compute_normalized_soft_discounted_cumulative_gain(df_anonym_sample['note'][0], df_anonym_sample['anonym_note'][0])\n",
    "print(f\"NSDCG@K Score: {nsdcg_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute metrics for all records in masked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T10:01:33.939057Z",
     "start_time": "2025-08-13T10:01:33.933250Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process the desired output.\n",
    "def process_metrics(id_note: int, ALID: float, LR: float, LRQI: float, JSC: float, NSDCG: float) -> dict:\n",
    "\n",
    "    return {\n",
    "    \"id_note\": int(id_note),\n",
    "    \"ALID\": round(float(ALID), 2),\n",
    "    \"LR\": round(float(LR), 2),\n",
    "    \"LRQI\": round(float(LRQI), 2),\n",
    "    \"JSC\": round(float(JSC), 2),\n",
    "    \"NSDCG\": round(float(NSDCG), 2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:09:35.943816Z",
     "start_time": "2025-08-13T13:57:46.794331Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note evaluation progress:   0%|          | 0/5000 [00:00<?, ?it/s]C:\\Users\\rafau\\AppData\\Local\\Temp\\ipykernel_11524\\2573721112.py:14: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  relevances = np.exp(logits_orig[top_k_indices_anon])\n",
      "Note evaluation progress: 100%|██████████| 5000/5000 [11:49<00:00,  7.05it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Check if exists test folder\n",
    "if 'test' not in os.listdir():\n",
    "    os.mkdir('test')\n",
    "\n",
    "# File to save masked note evaluation.\n",
    "file_path = f'./test/llama3_generalization_note_eval.json'\n",
    "\n",
    "# Check if file exists.\n",
    "if os.path.exists(file_path):\n",
    "    # Empty file.\n",
    "    if os.stat(file_path).st_size == 0:\n",
    "        formatted_notes = []\n",
    "    else:\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            formatted_notes = data.get('notes', [])\n",
    "else:\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump({\"notes\": []}, json_file)\n",
    "    formatted_notes = []\n",
    "\n",
    "# Recupera tutti gli id_note già presenti\n",
    "existing_ids = {note[\"id_note\"] for note in formatted_notes}\n",
    "\n",
    "# Iterate over sample dataset.\n",
    "for i in tqdm(range(0, len(df_anonym_sample)), desc=\"Note evaluation progress\"):\n",
    "\n",
    "    # Read note and masked_note to sample masked dataset.\n",
    "    note = df_anonym_sample['note'][i]\n",
    "    anonym_note = df_anonym_sample['anonym_note'][i]\n",
    "    sensitive_entity_note = ast.literal_eval(df_anonym_sample['sensitive_entity_note'][i])\n",
    "    index = df_anonym_sample['index'][i]\n",
    "\n",
    "    # ALID\n",
    "    ALID = compute_average_levenshtein_index_of_dissimilarity(anonym_note, sensitive_entity_note)\n",
    "\n",
    "    # LR\n",
    "    LR = compute_LR_ths(anonym_note, sensitive_entity_note)\n",
    "\n",
    "    # LRQI\n",
    "    LRQI = compute_LRQI_ths(anonym_note, sensitive_entity_note, df['summary'][index])\n",
    "\n",
    "    # JSC\n",
    "    JSC = compute_jaccard_similarity_coefficient(note, anonym_note)\n",
    "\n",
    "    # NSDCG\n",
    "    NSDCG = compute_normalized_soft_discounted_cumulative_gain(note, anonym_note)\n",
    "\n",
    "    # Process output.\n",
    "    metrics = process_metrics(id_note=index, ALID=ALID, LR=LR, LRQI=LRQI, JSC=JSC, NSDCG=NSDCG)\n",
    "    formatted_notes.append(metrics)\n",
    "\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump({\"notes\": formatted_notes}, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:10:58.379661Z",
     "start_time": "2025-08-14T14:10:58.351662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Generalization (Llama3): {'ALID': 55.102216000000006, 'LR': 93.626312, 'LRQI': 93.25791643760307, 'JSC': 17.512335999999998, 'NSDCG': 53.203804000000005}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = './test/llama3_generalization_note_eval.json'\n",
    "\n",
    "# Read json file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "notes = data[\"notes\"]\n",
    "\n",
    "metrics = [\"ALID\", \"LR\", \"LRQI\", \"JSC\", \"NSDCG\"]\n",
    "\n",
    "averages_mixed = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    if metric == \"LRQI\":\n",
    "        # Average only on values != -1\n",
    "        values = [entry[metric] for entry in notes if entry[\"LRQI\"] != -1]\n",
    "    else:\n",
    "        # Average on all entries\n",
    "        values = [entry[metric] for entry in notes]\n",
    "    averages_mixed[metric] = sum(values) / len(values) if values else None\n",
    "\n",
    "print('Data Generalization (Llama3):', averages_mixed)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
