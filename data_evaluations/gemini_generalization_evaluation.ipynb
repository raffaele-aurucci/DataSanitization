{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ax3t641WR_5"
   },
   "source": [
    "### **Gemini 2.5 Pro Data Generalization Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ZSpfjx701H-",
    "outputId": "04ced276-3c54-466a-b9c3-146f2ee9ad53"
   },
   "outputs": [],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hwey6TGLCC8N",
    "outputId": "f1ff3cc6-0a39-44dc-d3a8-36bd439a405e"
   },
   "outputs": [],
   "source": [
    "%pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:07:15.255474Z",
     "start_time": "2025-08-13T15:07:15.250819Z"
    },
    "id": "_QZktNg4ayNd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8HQzX5JPdfV"
   },
   "source": [
    "#### Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:08:19.053379Z",
     "start_time": "2025-08-13T15:08:09.771240Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"hf://datasets/AGBonnet/augmented-clinical-notes/augmented_notes_30K.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:07:28.891709Z",
     "start_time": "2025-08-13T15:07:28.636332Z"
    },
    "id": "Uz4QGKmYPZpf"
   },
   "outputs": [],
   "source": [
    "df_anonym_sample = pd.read_csv(\"../datasets/sample/gemini_generalization_dataset_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:08:21.001207Z",
     "start_time": "2025-08-13T15:08:20.986206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>note</th>\n",
       "      <th>anonym_note</th>\n",
       "      <th>sensitive_entity_note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2308</td>\n",
       "      <td>A 30-year-old man sustained an anterior disloc...</td>\n",
       "      <td>An adult patient sustained a shoulder injury w...</td>\n",
       "      <td>{'a coracoid osteotomy', 'anterior dislocation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22404</td>\n",
       "      <td>A 64-year-old Japanese man consulted our hospi...</td>\n",
       "      <td>An adult patient consulted a hospital due to a...</td>\n",
       "      <td>{'paroxysmal atrial fibrillation', '210 × 200 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23397</td>\n",
       "      <td>A 61 year old man presented to emergency room ...</td>\n",
       "      <td>An adult patient presented to the emergency ro...</td>\n",
       "      <td>{'80', '18/min', '3 days', '98-99%', '80/min',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25058</td>\n",
       "      <td>A 33-year-old woman presented at the emergency...</td>\n",
       "      <td>An adult patient presented at the emergency de...</td>\n",
       "      <td>{'MRI of the spine', 'intradural intramedullar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2664</td>\n",
       "      <td>A healthy 36-year-old woman with no past medic...</td>\n",
       "      <td>An adult patient with no significant past medi...</td>\n",
       "      <td>{'COVID-19 PCR test', 'attending thoracic surg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>25284</td>\n",
       "      <td>An 18-year-old male high school wrestler with ...</td>\n",
       "      <td>A young adult patient, an athlete, presented t...</td>\n",
       "      <td>{'fever of 101.9', 'erythrocyte sedimentation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>18355</td>\n",
       "      <td>A 61-year-old African American female was foun...</td>\n",
       "      <td>An adult patient was found unresponsive follow...</td>\n",
       "      <td>{'at the time they arrived', '“venous oozing”'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>27684</td>\n",
       "      <td>73-year-old female with history of hypertensio...</td>\n",
       "      <td>Elderly patient with a history of cardiovascul...</td>\n",
       "      <td>{'left femoral', 'on the same day of admission...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4110</td>\n",
       "      <td>A 52-year-old woman presented with pain and ac...</td>\n",
       "      <td>An adult individual presented with pain and re...</td>\n",
       "      <td>{'magnetic resonance imaging', 'glenoid cavity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>25473</td>\n",
       "      <td>A Syrian 7-year old girl was referred to us fo...</td>\n",
       "      <td>A young child from a Middle Eastern background...</td>\n",
       "      <td>{'february 2014', 'NICU', 'day 2', 'failure to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               note  \\\n",
       "0      2308  A 30-year-old man sustained an anterior disloc...   \n",
       "1     22404  A 64-year-old Japanese man consulted our hospi...   \n",
       "2     23397  A 61 year old man presented to emergency room ...   \n",
       "3     25058  A 33-year-old woman presented at the emergency...   \n",
       "4      2664  A healthy 36-year-old woman with no past medic...   \n",
       "...     ...                                                ...   \n",
       "4995  25284  An 18-year-old male high school wrestler with ...   \n",
       "4996  18355  A 61-year-old African American female was foun...   \n",
       "4997  27684  73-year-old female with history of hypertensio...   \n",
       "4998   4110  A 52-year-old woman presented with pain and ac...   \n",
       "4999  25473  A Syrian 7-year old girl was referred to us fo...   \n",
       "\n",
       "                                            anonym_note  \\\n",
       "0     An adult patient sustained a shoulder injury w...   \n",
       "1     An adult patient consulted a hospital due to a...   \n",
       "2     An adult patient presented to the emergency ro...   \n",
       "3     An adult patient presented at the emergency de...   \n",
       "4     An adult patient with no significant past medi...   \n",
       "...                                                 ...   \n",
       "4995  A young adult patient, an athlete, presented t...   \n",
       "4996  An adult patient was found unresponsive follow...   \n",
       "4997  Elderly patient with a history of cardiovascul...   \n",
       "4998  An adult individual presented with pain and re...   \n",
       "4999  A young child from a Middle Eastern background...   \n",
       "\n",
       "                                  sensitive_entity_note  \n",
       "0     {'a coracoid osteotomy', 'anterior dislocation...  \n",
       "1     {'paroxysmal atrial fibrillation', '210 × 200 ...  \n",
       "2     {'80', '18/min', '3 days', '98-99%', '80/min',...  \n",
       "3     {'MRI of the spine', 'intradural intramedullar...  \n",
       "4     {'COVID-19 PCR test', 'attending thoracic surg...  \n",
       "...                                                 ...  \n",
       "4995  {'fever of 101.9', 'erythrocyte sedimentation ...  \n",
       "4996  {'at the time they arrived', '“venous oozing”'...  \n",
       "4997  {'left femoral', 'on the same day of admission...  \n",
       "4998  {'magnetic resonance imaging', 'glenoid cavity...  \n",
       "4999  {'february 2014', 'NICU', 'day 2', 'failure to...  \n",
       "\n",
       "[5000 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anonym_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T13:42:04.344001Z",
     "start_time": "2025-08-13T13:42:04.337847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'150 kilometers from the main city',\n",
       " '3 months',\n",
       " '30-year-old man',\n",
       " '50° of forward flexion',\n",
       " 'COVID-19 status',\n",
       " 'GT fracture',\n",
       " 'L4 vertebral level of internal rotation',\n",
       " 'anterior dislocation of the right shoulder',\n",
       " 'computed tomography imaging',\n",
       " 'conjoint tendon',\n",
       " 'coracoid osteotomy',\n",
       " 'deltopectoral approach',\n",
       " 'fibrosis',\n",
       " 'full personal protective equipment precautions',\n",
       " 'general primary physicians',\n",
       " 'glenoid bone loss',\n",
       " 'last week of April 2020',\n",
       " 'lateral part of the proximal metaphysis of the proximal humerus',\n",
       " 'lesser tuberosity',\n",
       " 'local street fight',\n",
       " 'no X-ray imaging',\n",
       " 'ossification',\n",
       " 'primary health services',\n",
       " 'restricted use of the affected limb',\n",
       " 'severe medial displacement of the proximal humerus',\n",
       " 'severe pain',\n",
       " 'sling',\n",
       " 'subscapularis',\n",
       " 'visual analog scale VAS score of 8 points',\n",
       " '−10° of external rotation'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(df_anonym_sample['sensitive_entity_note'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEVFTfmZUBUL"
   },
   "source": [
    "### **Average Levenshtein Index of Dissimilarity (ALID)**\n",
    "<br>\n",
    "\n",
    "Compute LSI for each sensitive entity of observation:\n",
    "$$\n",
    "LSI = \\max_{j=1}^{L-e} LR_a(se_i, w_j)\n",
    "$$\n",
    "\n",
    "Next compute the average of each LSI calculated before:\n",
    "$$\n",
    "ALID = (1 - \\langle S \\rangle) \\times 100\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T13:42:30.979357Z",
     "start_time": "2025-08-13T13:42:30.973980Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXmS-pUrC8PM",
    "outputId": "6bf6ab4d-8abf-4a3c-cb01-426cb0efecf0"
   },
   "outputs": [],
   "source": [
    "def LRa(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    Levenshtein Ratio:\n",
    "    LRa(a, b) = 1 - LD(a, b) / max(len(a), len(b))\n",
    "    \"\"\"\n",
    "    len_a = len(a)\n",
    "    len_b = len(b)\n",
    "    if max(len_a, len_b) == 0:\n",
    "        return 1.0  # both strings are empty\n",
    "    ld = Levenshtein.distance(a, b)\n",
    "    return 1 - (ld / max(len_a, len_b))\n",
    "\n",
    "def compute_lsi(entity: str, text_anon: str) -> float:\n",
    "    e_len = len(entity)\n",
    "    max_lra = 0.0\n",
    "    for i in range(len(text_anon) - e_len + 1):\n",
    "        window = text_anon[i:i + e_len]\n",
    "        lra = LRa(entity, window)\n",
    "        max_lra = max(max_lra, lra)\n",
    "    return max_lra\n",
    "\n",
    "def compute_average_levenshtein_index_of_dissimilarity(anonym_note: str, sensitive_entity_note: list) -> float:\n",
    "    lsi_values = [compute_lsi(ent, anonym_note) for ent in sensitive_entity_note]\n",
    "    average_lsi = np.mean(lsi_values)\n",
    "    ALID = (1 - average_lsi) * 100\n",
    "    return ALID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T13:42:31.812289Z",
     "start_time": "2025-08-13T13:42:31.762376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALID: 51.77%\n"
     ]
    }
   ],
   "source": [
    "alid = compute_average_levenshtein_index_of_dissimilarity(df_anonym_sample['anonym_note'][0], ast.literal_eval(df_anonym_sample['sensitive_entity_note'][0]))\n",
    "print(f\"ALID: {alid:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3suaj0HUIxv"
   },
   "source": [
    "### **Levenshtein Recall (LR)**\n",
    "\n",
    "<br>\n",
    "To calculate LR, each LSI in S is compared to a selected similarity threshold, $ths$, set to 0.85:\n",
    "\n",
    "$$\n",
    "LR@ths = \\frac{ \\sum_{i=1}^{l} (S_i < ths) }{l} \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T13:42:44.150266Z",
     "start_time": "2025-08-13T13:42:44.145122Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b4K6-AEQFUA",
    "outputId": "05677b87-1490-4a46-ad32-98bc7c8cdcaf"
   },
   "outputs": [],
   "source": [
    "def compute_LR_ths(anonym_note: str, sensitive_entity_note: list, ths=0.85) -> float:\n",
    "    LSI_scores = []\n",
    "    for entity in sensitive_entity_note:\n",
    "        LSI_scores.append(compute_lsi(entity, anonym_note))\n",
    "\n",
    "    if len(sensitive_entity_note) == 0:\n",
    "        return -1\n",
    "\n",
    "    # Compute recall\n",
    "    num_deidentified = sum(1 for s in LSI_scores if s < ths)\n",
    "    recall = (num_deidentified / len(sensitive_entity_note)) * 100\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T13:42:46.158860Z",
     "start_time": "2025-08-13T13:42:46.109682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Recall: 86.67%\n"
     ]
    }
   ],
   "source": [
    "lr_score = compute_LR_ths(df_anonym_sample['anonym_note'][0], ast.literal_eval(df_anonym_sample['sensitive_entity_note'][0]))\n",
    "print(f\"Levenshtein Recall: {lr_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrpd13nOWG0q"
   },
   "source": [
    "### **Levenshtein Recall Quasi Identifier (LRQI)**\n",
    "\n",
    "The LRQI is calculated similarly to the LR but only considering quasi-identifiers.\n",
    "\n",
    "$$\n",
    "\\text{LRQI}@ths = \\frac{ \\sum_{k=1}^{l_{qi}} (S_k < ths) }{l_{qi}} \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T13:42:50.758363Z",
     "start_time": "2025-08-13T13:42:50.741405Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxfhW2ftWGik",
    "outputId": "71a794e6-a3d7-4a98-9ce5-42a862de0bcb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import difflib\n",
    "\n",
    "def compute_LRQI_ths(anonym_note: str, sensitive_entity_note: list, json_summary: str, ths=0.85) -> float:\n",
    "\n",
    "    def extract_qi_values_from_json(json_str: str) -> list:\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            return []\n",
    "\n",
    "        qi_values = []\n",
    "\n",
    "        # Extract patient information\n",
    "        pi = data.get(\"patient information\", {})\n",
    "        for key in [\"age\", \"sex\", \"ethnicity\", \"occupation\", \"socio economic context\"]:\n",
    "            v = pi.get(key)\n",
    "            if v and v.lower() != \"none\":\n",
    "                qi_values.append(v)\n",
    "\n",
    "        # Extract patient medical history\n",
    "        pmh = data.get(\"patient medical history\", {})\n",
    "\n",
    "        if isinstance(pmh, dict):\n",
    "\n",
    "            # Physiological context\n",
    "            phsy = pmh.get(\"physiological context\")\n",
    "            if phsy is not None:\n",
    "                if isinstance(phsy, str):\n",
    "                    phsy = phsy.split(\",\")\n",
    "                elif isinstance(phsy, dict):\n",
    "                    phsy = list(phsy.values())\n",
    "                else:\n",
    "                    phsy = []\n",
    "                for p in phsy:\n",
    "                    qi_values.append(p)\n",
    "\n",
    "            # Psychological context\n",
    "            psych = pmh.get(\"psychological context\")\n",
    "            if psych is not None:\n",
    "                if isinstance(psych, str):\n",
    "                    psych = psych.split(\",\")\n",
    "                elif isinstance(psych, dict):\n",
    "                    psych = list(psych.values())\n",
    "                else:\n",
    "                    psych = []\n",
    "                for p in psych:\n",
    "                    qi_values.append(p)\n",
    "\n",
    "        elif isinstance(pmh, str) and pmh.lower() != \"none\":\n",
    "            qi_values.append(pmh)\n",
    "\n",
    "        # Extract symptoms\n",
    "        for s in data.get(\"symptoms\", []):\n",
    "            if s is not None and isinstance(s, dict):\n",
    "                if s.get(\"time\") and s[\"time\"].lower() != \"none\":\n",
    "                    qi_values.append(s[\"time\"])\n",
    "                if s.get(\"location\") and s[\"location\"].lower() != \"none\":\n",
    "                    qi_values.append(s[\"location\"])\n",
    "                if s.get(\"name of symptom\") and s[\"name of symptom\"].lower() != \"none\":\n",
    "                    for symptom in s.get(\"name of symptom\").split(\",\"):\n",
    "                        qi_values.append(symptom)\n",
    "\n",
    "        # Extract treatments\n",
    "        for t in data.get(\"treatments\", []):\n",
    "            if t is not None and isinstance(t, dict):\n",
    "                if t.get(\"name\") and t[\"name\"].lower() != \"none\":\n",
    "                    qi_values.append(t[\"name\"])\n",
    "                if t.get(\"dosage\") and t[\"dosage\"].lower() != \"none\":\n",
    "                    qi_values.append(t[\"dosage\"])\n",
    "                if t.get(\"reaction to treatment\") and t[\"reaction to treatment\"].lower() != \"none\":\n",
    "                    for reaction in t.get(\"reaction to treatment\").split(\",\"):\n",
    "                        qi_values.append(reaction)\n",
    "                if t.get(\"time\") and t[\"time\"].lower() != \"none\":\n",
    "                    qi_values.append(t[\"time\"])\n",
    "        return qi_values\n",
    "\n",
    "\n",
    "    def filter_qi_entities(sensitive_entities, qi_values, threshold=0.5):\n",
    "        filtered = []\n",
    "        se_lower = [e.lower() for e in sensitive_entities]\n",
    "\n",
    "        for qi in qi_values:\n",
    "            qi_low = qi.lower()\n",
    "            for ent in se_lower:\n",
    "                if qi_low in ent or ent in qi_low:\n",
    "                    filtered.append(ent)\n",
    "\n",
    "        for qi in qi_values:\n",
    "            qi_low = qi.lower()\n",
    "            for se in sensitive_entities:\n",
    "                se_low = se.lower()\n",
    "                ratio = difflib.SequenceMatcher(None, qi_low, se_low).ratio()\n",
    "                if ratio >= threshold:\n",
    "                    filtered.append(se)\n",
    "\n",
    "        return list(set(filtered))\n",
    "\n",
    "    qi_vals = extract_qi_values_from_json(json_summary)\n",
    "\n",
    "    if len(qi_vals) == 0: return -1\n",
    "\n",
    "    qi_entities = filter_qi_entities(sensitive_entity_note, qi_vals)\n",
    "\n",
    "    return compute_LR_ths(anonym_note, qi_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T13:42:52.207487Z",
     "start_time": "2025-08-13T13:42:52.202455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Recall Quasi Identifiers: -1.00%\n"
     ]
    }
   ],
   "source": [
    "lr_score = compute_LRQI_ths(anonym_note=df_anonym_sample['anonym_note'][0], sensitive_entity_note=ast.literal_eval(df_anonym_sample['sensitive_entity_note'][0]),\n",
    "                            json_summary=df['summary'][df_anonym_sample['index'][0]])\n",
    "print(f\"Levenshtein Recall Quasi Identifiers: {lr_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mLErJpWUO5R"
   },
   "source": [
    "### **Jaccard Similarity Coefficient (JSC)**\n",
    "\n",
    "Let C11 be the number of classes where both representations have a value of 1 and C01 + C10 be the number of classes where the representations have different values:\n",
    "\n",
    "$$\n",
    "\\text{JSC@thb} = \\frac{C_{11}}{C_{11} + C_{01} + C_{10}} \\times 100\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T13:43:06.828915Z",
     "start_time": "2025-08-13T13:42:56.689999Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLh3mM4QC1rE",
    "outputId": "53456830-b761-4660-f09b-e7e84c9aee70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AkshatSurolia/ICD-10-Code-Prediction\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"AkshatSurolia/ICD-10-Code-Prediction\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "config = model.config\n",
    "\n",
    "def chunk_text_ids(text: str, max_tokens=512):\n",
    "    inputs = tokenizer(text, add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids']\n",
    "    max_len = max_tokens - 2  # reserved for special tokens\n",
    "    chunks = [input_ids[i:i+max_len] for i in range(0, len(input_ids), max_len)]\n",
    "    return chunks\n",
    "\n",
    "def aggregate_logits_from_chunks(text) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    chunks = chunk_text_ids(text, max_tokens=512)\n",
    "    total_logits = None\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # add CLS e SEP token id\n",
    "        input_ids = [tokenizer.cls_token_id] + chunk + [tokenizer.sep_token_id]\n",
    "        input_tensor = torch.tensor([input_ids])  # batch size 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tensor).logits.squeeze(0)  # [num_classes]\n",
    "\n",
    "        if total_logits is None:\n",
    "            total_logits = logits\n",
    "        else:\n",
    "            total_logits += logits  # sum logits\n",
    "\n",
    "    return total_logits\n",
    "\n",
    "def compute_jaccard_similarity_coefficient(original_note: str, anonym_note: str, thb: float = 0.0005) -> float:\n",
    "\n",
    "    final_logits_original_note = aggregate_logits_from_chunks(original_note)\n",
    "    final_logits_masked_note = aggregate_logits_from_chunks(anonym_note)\n",
    "\n",
    "    probs_original_note = F.softmax(final_logits_original_note, dim=0).squeeze()\n",
    "    probs_anonym_note = F.softmax(final_logits_masked_note, dim=0).squeeze()\n",
    "\n",
    "    # list of indexes\n",
    "    # results = final_logits_original_note.detach().cpu().numpy()[0].argsort()[::-1][:10]\n",
    "    # [config.id2label[ids] for ids in results]\n",
    "\n",
    "    predicted_original_note = set(torch.where(probs_original_note > thb)[0].tolist())\n",
    "    predicted_masked_note = set(torch.where(probs_anonym_note > thb)[0].tolist())\n",
    "\n",
    "    C11 = len(predicted_original_note & predicted_masked_note)\n",
    "    C01 = len(predicted_masked_note - predicted_original_note)\n",
    "    C10 = len(predicted_original_note - predicted_masked_note)\n",
    "\n",
    "    denom = C11 + C01 + C10\n",
    "\n",
    "    if denom == 0: return 100.0  # noone classed\n",
    "\n",
    "    return (C11 / denom) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T13:43:07.559890Z",
     "start_time": "2025-08-13T13:43:06.834923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSC Score: 16.00%\n"
     ]
    }
   ],
   "source": [
    "jsc_score = compute_jaccard_similarity_coefficient(df_anonym_sample['note'][0], df_anonym_sample['anonym_note'][0])\n",
    "print(f\"JSC Score: {jsc_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7l2OtVdOUUfl"
   },
   "source": [
    "### **Normalized Soft Discounted Cumulative Gain (NSDCG@K)**\n",
    "\n",
    "**SDCG@K (Soft Discounted Cumulative Gain)** is a metric used to evaluate the ranking quality of the logits produced by a model on anonymized notes compared to the original ones.\n",
    "\n",
    "$$\n",
    "\\text{SDCG@K} = \\sum_{i=1}^{K} sdi \\cdot reli\n",
    "$$\n",
    "\n",
    "- $sd_i$: soft discount factor at position *i*\n",
    "- $rel_i$: relevance of the item at position *i*\n",
    "\n",
    "Given a vector of **sorted (descending) logits** \\( s \\), the soft discount at position *i* is computed using the **softmax** function:\n",
    "\n",
    "$$\n",
    "sdi = \\frac{e^{s_i}}{\\sum_{j=1}^{N} e^{s_j}}\n",
    "$$\n",
    "\n",
    "Where $N$ is the number of ICD-10 classes.\n",
    "\n",
    "The relevance of an item at position *i* in the original logits $z$ (i.e., logits from the original note ranked according to the anonymized note) is defined as:\n",
    "\n",
    "$$\n",
    "reli = e^{z_i}\n",
    "$$\n",
    "\n",
    "**NSDCG@K (Normalized SDCG)** is obtained by dividing the SDCG of the anonymized note by the SDCG of the ideal/original note and expressing the result as a percentage:\n",
    "\n",
    "$$\n",
    "\\text{NSDCG@K} = \\frac{\\text{SDCG@K}}{\\text{ISDCG@K}} \\times 100\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T13:43:20.124329Z",
     "start_time": "2025-08-13T13:43:20.118017Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RPMYsFy6ohi",
    "outputId": "b14a8ea5-9837-4075-829e-0eb4059adc26"
   },
   "outputs": [],
   "source": [
    "def compute_normalized_soft_discounted_cumulative_gain(original_note: str, anonym_note: str, k=10) -> float:\n",
    "\n",
    "    logits_orig = aggregate_logits_from_chunks(original_note)\n",
    "    logits_anon = aggregate_logits_from_chunks(anonym_note)\n",
    "\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    # top-K indexes from logit of masked text\n",
    "    top_k_values, top_k_indices_anon = torch.topk(logits_anon, k)\n",
    "\n",
    "    # relevance: original logit ordered via top-K of masked text\n",
    "    relevances = np.exp(logits_orig[top_k_indices_anon])\n",
    "\n",
    "    # softmax discount of original logit ordered decreasing\n",
    "    sorted_logits_orig = np.sort(logits_orig)[::-1]\n",
    "    softmax_discounts = softmax(sorted_logits_orig[:k])\n",
    "\n",
    "    # compute SDCG\n",
    "    sdcg = np.sum(softmax_discounts * relevances.cpu().numpy())\n",
    "\n",
    "    # compute ISDCG (ideal)\n",
    "    ideal_relevances = np.exp(sorted_logits_orig[:k])\n",
    "    ideal_discounts = softmax(sorted_logits_orig[:k])\n",
    "    isdcg = np.sum(ideal_discounts * ideal_relevances)\n",
    "\n",
    "    nsdcg = (sdcg / isdcg) * 100 if isdcg != 0 else 0\n",
    "\n",
    "    return nsdcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T13:43:21.983374Z",
     "start_time": "2025-08-13T13:43:21.188374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSDCG@K Score: 15.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rafau\\AppData\\Local\\Temp\\ipykernel_12152\\2573721112.py:14: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  relevances = np.exp(logits_orig[top_k_indices_anon])\n"
     ]
    }
   ],
   "source": [
    "nsdcg_score = compute_normalized_soft_discounted_cumulative_gain(df_anonym_sample['note'][0], df_anonym_sample['anonym_note'][0])\n",
    "print(f\"NSDCG@K Score: {nsdcg_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute metrics for all records in masked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T13:45:01.101571Z",
     "start_time": "2025-08-13T13:45:01.097135Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process the desired output.\n",
    "def process_metrics(id_note: int, ALID: float, LR: float, LRQI: float, JSC: float, NSDCG: float) -> dict:\n",
    "\n",
    "    return {\n",
    "    \"id_note\": int(id_note),\n",
    "    \"ALID\": round(float(ALID), 2),\n",
    "    \"LR\": round(float(LR), 2),\n",
    "    \"LRQI\": round(float(LRQI), 2),\n",
    "    \"JSC\": round(float(JSC), 2),\n",
    "    \"NSDCG\": round(float(NSDCG), 2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T19:05:30.484664Z",
     "start_time": "2025-08-13T15:08:35.109853Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note evaluation progress:   0%|          | 0/5000 [00:00<?, ?it/s]C:\\Users\\rafau\\AppData\\Local\\Temp\\ipykernel_12152\\2573721112.py:14: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  relevances = np.exp(logits_orig[top_k_indices_anon])\n",
      "Note evaluation progress: 100%|██████████| 5000/5000 [3:56:55<00:00,  2.84s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Check if exists test folder\n",
    "if 'test' not in os.listdir():\n",
    "    os.mkdir('test')\n",
    "\n",
    "# File to save masked note evaluation.\n",
    "file_path = f'./test/gemini_generalization_note_eval.json'\n",
    "\n",
    "# Check if file exists.\n",
    "if os.path.exists(file_path):\n",
    "    # Empty file.\n",
    "    if os.stat(file_path).st_size == 0:\n",
    "        formatted_notes = []\n",
    "    else:\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            formatted_notes = data.get('notes', [])\n",
    "else:\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump({\"notes\": []}, json_file)\n",
    "    formatted_notes = []\n",
    "\n",
    "# Iterate over sample dataset.\n",
    "for i in tqdm(range(0, len(df_anonym_sample)), desc=\"Note evaluation progress\"):\n",
    "\n",
    "    # Read note and masked_note to sample masked dataset.\n",
    "    note = df_anonym_sample['note'][i]\n",
    "    anonym_note = df_anonym_sample['anonym_note'][i]\n",
    "    sensitive_entity_note = ast.literal_eval(df_anonym_sample['sensitive_entity_note'][i])\n",
    "    index = df_anonym_sample['index'][i]\n",
    "\n",
    "    # ALID\n",
    "    ALID = compute_average_levenshtein_index_of_dissimilarity(anonym_note, sensitive_entity_note)\n",
    "\n",
    "    # LR\n",
    "    LR = compute_LR_ths(anonym_note, sensitive_entity_note)\n",
    "\n",
    "    # LRQI\n",
    "    LRQI = compute_LRQI_ths(anonym_note, sensitive_entity_note, df['summary'][index])\n",
    "\n",
    "    # JSC\n",
    "    JSC = compute_jaccard_similarity_coefficient(note, anonym_note)\n",
    "\n",
    "    # NSDCG\n",
    "    NSDCG = compute_normalized_soft_discounted_cumulative_gain(note, anonym_note)\n",
    "\n",
    "    # Process output.\n",
    "    metrics = process_metrics(id_note=index, ALID=ALID, LR=LR, LRQI=LRQI, JSC=JSC, NSDCG=NSDCG)\n",
    "    formatted_notes.append(metrics)\n",
    "\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump({\"notes\": formatted_notes}, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T14:08:51.839828Z",
     "start_time": "2025-08-14T14:08:51.811798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Generalization (Gemini 2.5 Pro): {'ALID': 54.256457999999995, 'LR': 95.879554, 'LRQI': 95.27244803063458, 'JSC': 23.266234, 'NSDCG': 62.169774}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = './test/gemini_generalization_note_eval.json'\n",
    "\n",
    "# Read json file\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "notes = data[\"notes\"]\n",
    "\n",
    "metrics = [\"ALID\", \"LR\", \"LRQI\", \"JSC\", \"NSDCG\"]\n",
    "\n",
    "averages_mixed = {}\n",
    "\n",
    "for metric in metrics:\n",
    "    if metric == \"LRQI\":\n",
    "        # Average only on values != -1\n",
    "        values = [entry[metric] for entry in notes if entry[\"LRQI\"] != -1]\n",
    "    else:\n",
    "        # Average on all entries\n",
    "        values = [entry[metric] for entry in notes]\n",
    "    averages_mixed[metric] = sum(values) / len(values) if values else None\n",
    "\n",
    "print('Data Generalization (Gemini 2.5 Pro):', averages_mixed)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
